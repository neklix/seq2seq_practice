{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_translation_tutorial (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Hj0yKkNUmQ1i"
      ]
    },
    "kernelspec": {
      "display_name": "mlkit",
      "language": "python",
      "name": "mlkit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPiT26XQfJRE",
        "outputId": "53785a6e-aefd-449d-a1bb-252a8c973bb2"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 12 18:47:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COTPPmYpfJRG"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkSgEe-Jku-r"
      },
      "source": [
        "# Seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNNAb0a9kuEE"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3v2Ww3lkf9x"
      },
      "source": [
        "В этом проекте мы будем учиться делать перевод с французского на английский. Примерно так:\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "Для этого мы будем исплользовать мощную идею «sequence-to-sequence» сетей (https://arxiv.org/abs/1409.3215), в которых две рекуррентные сети обучаются вместе для преоразования одной последовательности в другую.\n",
        "\n",
        "* Encoder-сеть сжимает входную последовательность в вектор.\n",
        "* Decoder-сеть разжимает этот вектор в новую последовательность.\n",
        "\n",
        "Всё как с автоэнкодерами, только encoder и decoder из разных доменов.\n",
        "\n",
        "Чтобы вся эта схема обучалась стабильнее, мы будем использовать механизм attention (https://arxiv.org/abs/1409.0473), позволяющий декодеру «фокусироваться» на специфичных токенах входной последовательности.\n",
        "\n",
        "**Рекомендуемое чтение:**\n",
        "\n",
        "-  Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "   Statistical Machine Translation (https://arxiv.org/abs/1406.1078)\n",
        "-  Sequence to Sequence Learning with Neural\n",
        "   Networks (https://arxiv.org/abs/1409.3215)\n",
        "-  Neural Machine Translation by Jointly Learning to Align and\n",
        "   Translate 9https://arxiv.org/abs/1409.0473)\n",
        "-  A Neural Conversational Model (https://arxiv.org/abs/1506.05869>)\n",
        "\n",
        "Если кто-то пропустил предыдущие занатия, то лучше сначала сделать их: основные концепции такие же, как в языковых моделях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt1ZcvSZkf9y"
      },
      "source": [
        "# осторожно: тетрадка старая\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A9f1xpMkf90"
      },
      "source": [
        "## Данные\n",
        "\n",
        "В этом проекте мы будем работать с кучей пар предложений на английском и французском.\n",
        "\n",
        "Скачайте данные отсюда (https://download.pytorch.org/tutorial/data.zip) и возьмите оттуда файлик eng-fra.txt. В нём должно быть много строчек примерно такого формата:\n",
        "\n",
        "    I am cold.    J'ai froid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_oxaMQJzA_t",
        "outputId": "e4b2d81a-1613-4e0c-b9f7-153c47cd0c57"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-12 18:47:31--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 99.86.33.56, 99.86.33.87, 99.86.33.64, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|99.86.33.56|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-05-12 18:47:31 (67.9 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qng3p34fzR93",
        "outputId": "36db5ee1-2b0c-45f0-ba39-b5b331191adb"
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEzTbBF5zVE4",
        "outputId": "59f0206d-059d-423a-deec-3bbbc65f8106"
      },
      "source": [
        "!wc -l data/eng-fra.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "135842 data/eng-fra.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8kamhi1fJRJ",
        "outputId": "4930e5bd-89c6-49ba-bb04-03b4892fb833"
      },
      "source": [
        "! head data/eng-fra.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Stop!\tÇa suffit !\n",
            "Stop!\tStop !\n",
            "Stop!\tArrête-toi !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pAoNlcCkf92"
      },
      "source": [
        "Делать предобработку будем по аналогии с char-level RNN-ками из предыдущих туториалов, только на этот раз нам важно отдельно запариться с EOS (end-of-sequence) — специальным токеном, который сеть будет генерировать при окончании генерации предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYgxYCIZkf93"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "    def __repr__(self):\n",
        "        most_popular_words = sorted(\n",
        "            self.word2count.keys(), key=lambda word: self.word2count[word], reverse=True\n",
        "        )[:10]\n",
        "        most_popular_words = \", \".join(most_popular_words)\n",
        "        return f\"Language: {self.name} | Num words: {self.n_words} | Most popular: {most_popular_words}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G--d0a55kf95"
      },
      "source": [
        "Все файлы в юникоде. Чтобы  облегчить нам работу, мы переведем все в ASCII, сделаем lowercase и выкинем большинство пунктуации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygu9l499kf96"
      },
      "source": [
        "# \"hello!\" -> hello, ! \n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nExIwuorkf98"
      },
      "source": [
        "При чтении данных разделим файл на строки, а строки на пары.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR6MaACekf99"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byoIUUdFkf9_"
      },
      "source": [
        "Данных у нас **много**, но мы сейчас хотим обучить что-нибудь по-быстрому, поэтому оставим оттуда только короткие и простые предложения — с длиной до 10 слов (включая пунктуацию)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rE0V5VGkf-A"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSo1cu0rkf-C"
      },
      "source": [
        "Полный процесс такой:\n",
        "\n",
        "- Считать текстовый файл, просплитить по линиям, а затем по парам.\n",
        "- Нормализовать текст, профильтровать по длине.\n",
        "- Сделать готовые списки слов из сырых предложений в каждом языке."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G_BKtnukf-D",
        "outputId": "36e8db7b-774f-40a2-def5-4629c196226b"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['je suis sidere .', 'i m stunned .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrZ6mb1AfJRL",
        "outputId": "9ecc1f0b-24c1-46bc-fe95-bbc0fa2c4930"
      },
      "source": [
        "input_lang"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Language: fra | Num words: 4345 | Most popular: ., je, suis, est, vous, pas, de, il, nous, tu"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1McY4a2fJRL",
        "outputId": "ce9ca05d-f5fd-410d-a120-a42abe6cbaa3"
      },
      "source": [
        "output_lang"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Language: eng | Num words: 2803 | Most popular: ., i, re, you, m, he, not, is, a, to"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9KPbYdwfJRM",
        "outputId": "0ba09637-14d3-49ad-e6f3-95ba16d3013f"
      },
      "source": [
        "len(pairs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFuRIIC2fJRM",
        "outputId": "26b98877-3b61-4a4a-f0df-d9ed877df113"
      },
      "source": [
        "pairs[24]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['je vais bien .', 'i m fine .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmijlK-akf-G"
      },
      "source": [
        "![Screenshot%202020-12-19%20at%2016.44.09.png](attachment:Screenshot%202020-12-19%20at%2016.44.09.png)The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzGBS-alkf-H"
      },
      "source": [
        "![Screenshot%202020-12-19%20at%2016.44.54.png](attachment:Screenshot%202020-12-19%20at%2016.44.54.png)The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpcrvvXakf-I"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # num_embedding = vocab_size_fra\n",
        "        self.embedder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # (batch_size, num_words) -> (batch_size, num_words, dim_1)\n",
        "        embeddings = self.embedder(input).view(1, 1, -1)\n",
        "        # (batch_size, num_words, dim_2)\n",
        "        output, hidden = self.gru(embeddings, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoWSf2ngfJRN"
      },
      "source": [
        "tokens = torch.randint(0, 1000, size=(128, 40))\n",
        "embedder = nn.Embedding(1000, 128)  # здесь лежит матрица"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tXyQQWcfJRN"
      },
      "source": [
        "onehot = torch.nn.functional.one_hot(tokens, num_classes=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSVNPXxAfJRN"
      },
      "source": [
        "embeddingds_first_way = embedder(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNZcay6Hkf-K"
      },
      "source": [
        "The Decoder\n",
        "-----------\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJW6eKikf-L"
      },
      "source": [
        "![Screenshot%202020-12-19%20at%2016.45.11.png](attachment:Screenshot%202020-12-19%20at%2016.45.11.png)Simple Decoder\n",
        "^^^^^^^^^^^^^^\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtM1ZGRskf-M"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedder = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # (batch_size, num_words, dim)\n",
        "        # (1, 1, num_words * dim)\n",
        "        output = self.embedder(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        # (batch_size, num_words, dim) -> (batch_size, num_words, num_classes)\n",
        "        # (batch_size, num_words, vocab_size_eng)\n",
        "        output = self.out(output[0])\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfTzzPptkf-O"
      },
      "source": [
        "I encourage you to train and observe the results of this model, but to\n",
        "save space we'll be going straight for the gold and introducing the\n",
        "Attention Mechanism.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNllTJbIkf-P"
      },
      "source": [
        "![Screenshot%202020-12-19%20at%2016.45.28.png](attachment:Screenshot%202020-12-19%20at%2016.45.28.png)Attention Decoder\n",
        "^^^^^^^^^^^^^^^^^\n",
        "\n",
        "If only the context vector is passed betweeen the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1wiuMhmfJRO"
      },
      "source": [
        "# v^TWm\n",
        "# U^T tanh (W_1 v + W_2 m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZwO_Gg2kf-P"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(\n",
        "                torch.cat((embedded[0], hidden[0]), 1)\n",
        "            ), \n",
        "            dim=1\n",
        "        )\n",
        "        attn_applied = torch.bmm(\n",
        "            attn_weights.unsqueeze(0),\n",
        "            encoder_outputs.unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLtuThIQkf-R"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
        "  limitation by using a relative position approach. Read about \"local\n",
        "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
        "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmb8OwO7kf-S"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3OzldY0kf-U"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na-c3f0nkf-V"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(\n",
        "    input_tensor, \n",
        "    target_tensor,\n",
        "    encoder, \n",
        "    decoder, \n",
        "    encoder_optimizer,\n",
        "    decoder_optimizer, \n",
        "    criterion,\n",
        "    max_length=MAX_LENGTH\n",
        "):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        # y_true: [sos, i, love, pizza, eos]\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # [0.9, 0.1, 0.0]\n",
        "            # [1, 0, 0]\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # beam_search is betters\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                # y_true: [sos, i, eos]\n",
        "                # [sos, i, eos, love]\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfmlhEzDkf-X"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNbWUTxfkf-Y"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc8pY8ZCkf-a"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skZhUHw1kf-a"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT4JmR_fkf-d"
      },
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf3Fhj-5kf-e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgHOJ5Ugkf-j"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYgjVG_kf-j"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Xly1n3kf-l"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58m1z4kJkf-m"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrSEC280kf-o"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBS0Q3gokf-p"
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "a_wUdnU1kf-s",
        "outputId": "9dbe39ad-7e60-4449-dad1-edee5bafd612"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> j emploie un nouvel ordinateur .\n",
            "= i am using a new computer .\n",
            "< i m going to a a . <EOS>\n",
            "\n",
            "> ce ne sont pas mes prisonniers .\n",
            "= they re not my prisoners .\n",
            "< i m not my my . <EOS>\n",
            "\n",
            "> vous etes tres braves .\n",
            "= you re very brave .\n",
            "< you re very sophisticated . <EOS>\n",
            "\n",
            "> je suis ici n est ce pas ?\n",
            "= i m here aren t i ?\n",
            "< i m here aren t you ? <EOS>\n",
            "\n",
            "> je ne suis sure de rien .\n",
            "= i m not sure of anything .\n",
            "< i m afraid to see you . <EOS>\n",
            "\n",
            "> tu ne saisis pas .\n",
            "= you re missing the point .\n",
            "< you re not a . <EOS>\n",
            "\n",
            "> t es un drole de gars .\n",
            "= you re a funny guy .\n",
            "< you re a funny . <EOS>\n",
            "\n",
            "> tu analyses trop .\n",
            "= you re over analyzing .\n",
            "< you re too . <EOS>\n",
            "\n",
            "> tu es stupide de lui faire confiance .\n",
            "= you re stupid to trust him .\n",
            "< you re out of me . <EOS>\n",
            "\n",
            "> c est l un de mes voisins .\n",
            "= he is one of my neighbours .\n",
            "< he is one of my . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4xW4kg-kf-u"
      },
      "source": [
        "Visualizing Attention\n",
        "---------------------\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuK-lqnc_30r"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "hZLCt827kf-v",
        "outputId": "f6f9d4b9-1b4a-4f83-980e-b73d1273698e"
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAECCAYAAAAGtFvhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAALrklEQVR4nO3df6jd9X3H8de7yW3SWJmOuTGNm/7Rdriy6rhoN8dg2k27SvvXQKH9Ywzyz7rZUSjt/tv/o3R/lEGwroO6yrAWitimjlqksLn6I2uNsUWcrb+GymxrN6aJvvdHrmC9mfd8snPu93uTxwOCuSeXw4tPTJ75nnPuPdXdAYARb5l6AAA7j3gAMEw8ABgmHgAMEw8AhokHAMNmG4+quraqvldVj1bVJ6feM0dVdWFV3V1VD1fVkaq6cepNc1ZVu6rqwaq6Y+otc1ZV51TVbVX1SFUdrarfmnrTHFXVX2z8uXuoqr5YVXun3rSdZhmPqtqV5LNJ3p/kkiQ3VNUl066apeNJPt7dlyR5b5I/dU5v6sYkR6cesQP8TZKvdfevJXlPnNkmVXVBkj9Pst7d706yK8n1067aXrOMR5LLkzza3Y9198tJbk3yoYk3zU53P9PdD2z8/MWc+EN+wbSr5qmq9if5QJKbpt4yZ1X1c0l+N8nnkqS7X+7uH006ar52J3lbVe1Osi/J0xPv2VZzjccFSZ543cdPxl+Kb6qqLkpyWZJ7J54yV59J8okkr068Y+4uTvJckr/beIjvpqo6a+pRc9PdTyX56yQ/TPJMkh9399enXbW95hoPBlTV25N8KcnHuvsnU++Zm6q6Lsmz3X3/1Ft2gN1JfjPJ33b3ZUn+K4nnHN+gqs7NiUdDLk5yfpKzqurD067aXnONx1NJLnzdx/s3buMNqmotJ8JxS3ffPvWemboyyQer6vGceAj0qqr6wrSTZuvJJE9292tXsLflREz4We9L8u/d/Vx3H0tye5LfnnjTtpprPL6d5B1VdXFVvTUnnoj6ysSbZqeqKicemz7a3Z+ees9cdfenunt/d1+UE/8vfaO7z6h/JS6qu/8jyRNV9a6Nm65O8vCEk+bqh0neW1X7Nv4cXp0z7IUFu6cecDLdfbyqPprkUE68iuHm7j4y8aw5ujLJR5J8t6oOb9z2l91953STOA38WZJbNv7h9liSP554z+x0971VdVuSB3LiVY8PJjk47artVb4lOwCj5vqwFQAzJh4ADBMPAIaJBwDDxAOAYbOOR1UdmHrDTuGsFuOcFuOcFnemntWs45HkjPxNOUXOajHOaTHOaXFn5FnNPR4AzNBKvkjwrbWn9+b//404j+WlrGXPEhYl7/yN/17K/SzT97+zb2n3tcyzOp05p8U4p8Wd7mf1Yl54vrvPe+PtK/n2JHtzVq6oq1dx16fs0KHDU0/Y5JrzL516AsCb+qe+7Qcnu93DVgAMEw8AhokHAMPEA4Bh4gHAMPEAYJh4ADBMPAAYJh4ADBMPAIaJBwDDxAOAYeIBwLCF4lFV11bV96rq0ar65KpHATBvW8ajqnYl+WyS9ye5JMkNVXXJqocBMF+LXHlcnuTR7n6su19OcmuSD612FgBztkg8LkjyxOs+fnLjNgDOUEt7J8GqOpCNN4Lfm+W9vSoA87PIlcdTSS583cf7N277Gd19sLvXu3v9dH4/XwAWi8e3k7yjqi6uqrcmuT7JV1Y7C4A52/Jhq+4+XlUfTXIoya4kN3f3kZUvA2C2FnrOo7vvTHLnircAsEP4CnMAhokHAMPEA4Bh4gHAMPEAYJh4ADBMPAAYJh4ADBMPAIaJBwDDxAOAYeIBwLClvRnU3F1z/qVTT9gxDj19eOoJm/j9g3lx5QHAMPEAYJh4ADBMPAAYJh4ADBMPAIaJBwDDxAOAYeIBwDDxAGCYeAAwTDwAGCYeAAwTDwCGiQcAw7aMR1XdXFXPVtVD2zEIgPlb5Mrj80muXfEOAHaQLePR3fck+c9t2ALADuE5DwCGLe09zKvqQJIDSbI3+5Z1twDM0NKuPLr7YHevd/f6WvYs624BmCEPWwEwbJGX6n4xyT8neVdVPVlVf7L6WQDM2ZbPeXT3DdsxBICdw8NWAAwTDwCGiQcAw8QDgGHiAcAw8QBgmHgAMEw8ABgmHgAMEw8AhokHAMPEA4Bh4gHAsKW9kyCnj2vOv3TqCZscevrw1BM2meM5wXZx5QHAMPEAYJh4ADBMPAAYJh4ADBMPAIaJBwDDxAOAYeIBwDDxAGCYeAAwTDwAGCYeAAwTDwCGbRmPqrqwqu6uqoer6khV3bgdwwCYr0Xez+N4ko939wNVdXaS+6vqru5+eMXbAJipLa88uvuZ7n5g4+cvJjma5IJVDwNgvoae86iqi5JcluTelawBYEdY+G1oq+rtSb6U5GPd/ZOT/PqBJAeSZG/2LW0gAPOz0JVHVa3lRDhu6e7bT/Y53X2wu9e7e30te5a5EYCZWeTVVpXkc0mOdvenVz8JgLlb5MrjyiQfSXJVVR3e+PGHK94FwIxt+ZxHd38rSW3DFgB2CF9hDsAw8QBgmHgAMEw8ABgmHgAMEw8AhokHAMPEA4Bh4gHAMPEAYJh4ADBMPAAYJh4ADBMPAIaJBwDDxAOAYeIBwDDxAGCYeAAwTDwAGCYeAAwTDwCGiQcAw8QDgGHiAcAw8QBgmHgAMEw8ABi2ZTyqam9V/WtV/VtVHamqv9qOYQDM1+4FPuelJFd190+rai3Jt6rqq939LyveBsBMbRmP7u4kP934cG3jR69yFADzttBzHlW1q6oOJ3k2yV3dfe9KVwEwawvFo7tf6e5Lk+xPcnlVvfuNn1NVB6rqvqq671heWvJMAOZk6NVW3f2jJHcnufYkv3awu9e7e30te5Y0D4A5WuTVVudV1TkbP39bkt9P8siKdwEwY4u82uqXk/x9Ve3Kidj8Y3ffsdpZAMzZIq+2+k6Sy7ZhCwA7hK8wB2CYeAAwTDwAGCYeAAwTDwCGiQcAw8QDgGHiAcAw8QBgmHgAMEw8ABgmHgAMEw8Ahi3yLdlPzVt2reyuT0m/OvWCTWrXzM5oQx8/PvWETa79lfWpJ2xy3ZHnpp6wyR2/fu7UEzbZde78NiVJdq/ur79T9eoLL0w9YbNjJ7/ZlQcAw8QDgGHiAcAw8QBgmHgAMEw8ABgmHgAMEw8AhokHAMPEA4Bh4gHAMPEAYJh4ADBMPAAYJh4ADFs4HlW1q6oerKo7VjkIgPkbufK4McnRVQ0BYOdYKB5VtT/JB5LctNo5AOwEi155fCbJJ5L8n+/lWlUHquq+qrrvWF5axjYAZmrLeFTVdUme7e773+zzuvtgd6939/pa9ixtIADzs8iVx5VJPlhVjye5NclVVfWFla4CYNa2jEd3f6q793f3RUmuT/KN7v7wypcBMFu+zgOAYbtHPrm7v5nkmytZAsCO4coDgGHiAcAw8QBgmHgAMEw8ABgmHgAMEw8AhokHAMPEA4Bh4gHAMPEAYJh4ADBMPAAYNvRddRfVZ+/L8SsuXcVdn7K1e7479YSdo2rqBZv0K69MPWGTr/7RFVNPOInvTz1gkx+/751TTzip//n5+f3b+bzPPzD1hM2Onfzm+Z0eALMnHgAMEw8AhokHAMPEA4Bh4gHAMPEAYJh4ADBMPAAYJh4ADBMPAIaJBwDDxAOAYeIBwLCFviV7VT2e5MUkryQ53t3rqxwFwLyNvJ/H73X38ytbAsCO4WErAIYtGo9O8vWqur+qDqxyEADzt+jDVr/T3U9V1S8muauqHunue17/CRtROZAke/aes9yVAMzKQlce3f3Uxn+fTfLlJJef5HMOdvd6d6+vrZ213JUAzMqW8aiqs6rq7Nd+nuQPkjy06mEAzNciD1v9UpIvV9Vrn/8P3f21la4CYNa2jEd3P5bkPduwBYAdwkt1ARgmHgAMEw8AhokHAMPEA4Bh4gHAMPEAYJh4ADBMPAAYJh4ADBMPAIaJBwDDxAOAYdXdy7/TqueS/GAJd/ULSZ5fwv2cCZzVYpzTYpzT4k73s/rV7j7vjTeuJB7LUlX3dff61Dt2Ame1GOe0GOe0uDP1rDxsBcAw8QBg2NzjcXDqATuIs1qMc1qMc1rcGXlWs37OA4B5mvuVBwAzJB4ADBMPAIaJBwDDxAOAYf8L8IhQVTA+flYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 480x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3H7D1wWkf-x"
      },
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dI3BNbbVkf-x",
        "outputId": "a40e98f6-26ea-4d49-86ce-6a2d0b2deb6b"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = elle a cinq ans de moins que moi .\n",
            "output = she is two years than me . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/mnt/chatbot_models2/fursov/tasks/ENV/virtualenvs/mlkit-32Y97wqZ-py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/mnt/chatbot_models2/fursov/tasks/ENV/virtualenvs/mlkit-32Y97wqZ-py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = elle est trop petit .\n",
            "output = she is too too . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = je ne crains pas de mourir .\n",
            "output = i m not afraid to <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = c est un jeune directeur plein de talent .\n",
            "output = he is a man man . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAtmD9bFfJRS"
      },
      "source": [
        "data = [\n",
        "    (\"где поесть?\", \"здесь\"),\n",
        "    (\"что сделать?\", \"ничего\")\n",
        "]\n",
        "\n",
        "# что делать, если такого слова не видели\n",
        "# @fursov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACG4Q06bkf-z"
      },
      "source": [
        "Exercises\n",
        "=========\n",
        "\n",
        "-  Try with a different dataset\n",
        "\n",
        "   -  Another language pair\n",
        "   -  Human → Machine (e.g. IOT commands)\n",
        "   -  Chat → Response\n",
        "   -  Question → Answer\n",
        "\n",
        "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
        "   GloVe\n",
        "-  Try with more layers, more hidden units, and more sentences. Compare\n",
        "   the training time and results.\n",
        "-  If you use a translation file where pairs have two of the same phrase\n",
        "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
        "   this:\n",
        "\n",
        "   -  Train as an autoencoder\n",
        "   -  Save only the Encoder network\n",
        "   -  Train a new Decoder for translation from there\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEUGQIDpI4bz"
      },
      "source": [
        "# New Dataset == New Life"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpLBIn2i6O4i"
      },
      "source": [
        "> Тяжело найти интересный датасет. Я нашёл датасет SQUAD_v2.0 (Текст, вопрос по тексту, ответ на вопрос), но я боюсь, что это слишком сложный датасет, так как надо именно в тексте искать ответы на вопрос. Есть ещё датасет нормализации русского текста (чиселки цифрами в чиселки текстами перевести с учётом форм слов по контексту). Наверное, попробую такой датасет. Он, скорее всего, может быть детерменированно по правилам языка может быть решён, но я попробую через seq2seq. А squad покопаю на досуге. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfd7vmeR6N_J",
        "outputId": "2d719d1d-3838-4437-808b-5c26bb2d2fac"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wVsj3b26UQZ"
      },
      "source": [
        "data_table = pd.read_csv('/content/drive/MyDrive/ru_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "XVpqAS_I77I9",
        "outputId": "63bbafcb-40c9-4c25-ccd1-8ff1f24a36fa"
      },
      "source": [
        "data_table.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>class</th>\n",
              "      <th>before</th>\n",
              "      <th>after</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>По</td>\n",
              "      <td>По</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>состоянию</td>\n",
              "      <td>состоянию</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>на</td>\n",
              "      <td>на</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>DATE</td>\n",
              "      <td>1862 год</td>\n",
              "      <td>тысяча восемьсот шестьдесят второй год</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>Оснащались</td>\n",
              "      <td>Оснащались</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>латными</td>\n",
              "      <td>латными</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>рукавицами</td>\n",
              "      <td>рукавицами</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>и</td>\n",
              "      <td>и</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>сабатонами</td>\n",
              "      <td>сабатонами</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>с</td>\n",
              "      <td>с</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>не</td>\n",
              "      <td>не</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>длинными</td>\n",
              "      <td>длинными</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>носками</td>\n",
              "      <td>носками</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>В</td>\n",
              "      <td>В</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>конце</td>\n",
              "      <td>конце</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>DATE</td>\n",
              "      <td>1811 года</td>\n",
              "      <td>тысяча восемьсот одиннадцатого года</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>вследствие</td>\n",
              "      <td>вследствие</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentence_id  token_id  ...      before                                   after\n",
              "0             0         0  ...          По                                      По\n",
              "1             0         1  ...   состоянию                               состоянию\n",
              "2             0         2  ...          на                                      на\n",
              "3             0         3  ...    1862 год  тысяча восемьсот шестьдесят второй год\n",
              "4             0         4  ...           .                                       .\n",
              "5             1         0  ...  Оснащались                              Оснащались\n",
              "6             1         1  ...     латными                                 латными\n",
              "7             1         2  ...  рукавицами                              рукавицами\n",
              "8             1         3  ...           и                                       и\n",
              "9             1         4  ...  сабатонами                              сабатонами\n",
              "10            1         5  ...           с                                       с\n",
              "11            1         6  ...          не                                      не\n",
              "12            1         7  ...    длинными                                длинными\n",
              "13            1         8  ...     носками                                 носками\n",
              "14            1         9  ...           .                                       .\n",
              "15            2         0  ...           В                                       В\n",
              "16            2         1  ...       конце                                   конце\n",
              "17            2         2  ...   1811 года     тысяча восемьсот одиннадцатого года\n",
              "18            2         3  ...           ,                                       ,\n",
              "19            2         4  ...  вследствие                              вследствие\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3HAGLG6JfHp"
      },
      "source": [
        "> Пристально смотрим на наш датасет. Мы его сейчас немного изменим. Мы возьмём некоторый набор классов (*'class'* column) и в *X* они будут как в *'before'*, а в *target* как в *'after'*. Остальные классы мы оставим без изменений.\n",
        "Какие классы мы изменим:\n",
        "\n",
        "\n",
        "\n",
        "*   **DATE**: 17 июля 2014 -> семнадцатого июля две тысячи четырнадцатого года\n",
        "*   **ORDINAL**: 7-й -> седьмой\n",
        "*   **MEASURE**: 480 с. -> четыреста восемьдесят секунд\n",
        "*   **CARDINAL**: 12 -> двенадцать\n",
        "\n",
        "Всё остальное просто копируем из *before*. Ну и поделим всё по предложениям, конечно же."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-CqKpjhNMf3"
      },
      "source": [
        "def build_sentences(dataset, att_classes):\n",
        "    sents_before = []\n",
        "    sents_after = []\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        if len(sents_before) <= dataset.loc[i, 'sentence_id']:\n",
        "            sents_before.append(['<START>'])\n",
        "            sents_after.append(['<START>'])\n",
        "        cur_id = dataset.loc[i, 'sentence_id']\n",
        "        if dataset.loc[i, 'class'] in att_classes:\n",
        "            sents_before[cur_id].append(str(dataset.loc[i, 'before']))\n",
        "            sents_after[cur_id].append(str(dataset.loc[i, 'after'].lower()))\n",
        "        else:\n",
        "            sents_before[cur_id].append(str(dataset.loc[i, 'before']).lower())\n",
        "            sents_after[cur_id].append(str(dataset.loc[i, 'before']).lower())\n",
        "    for i in range(len(sents_after)):\n",
        "        sents_before[i].append('<END>')\n",
        "        sents_after[i].append('<END>')\n",
        "    return sents_before, sents_after"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBuZM6saNMch",
        "outputId": "f83374f6-2698-4e1d-af27-b7685510e59f"
      },
      "source": [
        "dataset, target = build_sentences(data_table.loc[:1000000], ['DATE', 'ORDINAL', 'MEASURE', 'CARDINAL'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000001/1000001 [01:21<00:00, 12284.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-5GMvi1NMac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98d838d-ed70-4d30-ceff-368432d693fa"
      },
      "source": [
        "(' '.join(dataset[22]), ' '.join(target[22]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<START> 7 октября 2010 года территория республики конго отошла к новоучрежденной браззавильской и габонской епархии . <END>',\n",
              " '<START> седьмого октября две тысячи десятого года территория республики конго отошла к новоучрежденной браззавильской и габонской епархии . <END>')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL7m6U4N-Uk_"
      },
      "source": [
        "> В этой домашке батчей не будет (ну они будут размера 1). Надо написать класс Vocab. Наверное, он будет один общий для dataset и target, так как словари не сильно отличаются по факту."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B5HxO6JNMYf"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, data_X, data_y):\n",
        "        dataset = data_X + data_y\n",
        "        self.idx2word = list(set(itertools.chain(*dataset)))\n",
        "        self.idx2word.append('<UNK>')\n",
        "        self.word2idx = {}\n",
        "        for i, word in tqdm(enumerate(self.idx2word)):\n",
        "            self.word2idx[word] = i\n",
        "        self.data_X = [self.tokenize(i) for i in tqdm(data_X)]\n",
        "        self.data_y = [self.tokenize(i) for i in tqdm(data_y)]\n",
        "    \n",
        "    def tokenize(self, sequence):\n",
        "        return [self.word2idx[i] for i in sequence]\n",
        "    \n",
        "    def detokenize(self, sequence):\n",
        "        return [self.idx2word[i] for i in sequence]\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        return (self.data_X[key], self.data_y[key])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbCyhOP2aL-x"
      },
      "source": [
        "> Это больше класс датасета получился, чем класс словаря. Ну и ладно"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGijylLENMXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b758e2a-04b3-4e7f-9772-de350532f574"
      },
      "source": [
        "data = Vocab(dataset[:1050], target[:1050])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7342it [00:00, 1569950.55it/s]\n",
            "100%|██████████| 1050/1050 [00:00<00:00, 208365.78it/s]\n",
            "100%|██████████| 1050/1050 [00:00<00:00, 244369.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMapPr4IMmSt",
        "outputId": "152b3a73-2210-4c06-cd90-0b2124e9598a"
      },
      "source": [
        "len(data.idx2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7342"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlGagF7jNMVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6212a132-86d4-4d43-f962-f843848a1a61"
      },
      "source": [
        "data.detokenize(data[666][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START>',\n",
              " 'мукополисахаридозы',\n",
              " 'типов',\n",
              " 'три',\n",
              " ',',\n",
              " 'iv',\n",
              " ',',\n",
              " 'vi',\n",
              " ',',\n",
              " 'семь',\n",
              " '.',\n",
              " '<END>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8hQ3sZzYkHY"
      },
      "source": [
        "data_ = pd.read_csv('/content/drive/MyDrive/ru_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGtQBFhKYkFQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "a0ea7425-7248-402f-d6e0-6333573f6ab9"
      },
      "source": [
        "data_[data_['sentence_id'] == 666]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token_id</th>\n",
              "      <th>class</th>\n",
              "      <th>before</th>\n",
              "      <th>after</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9045</th>\n",
              "      <td>666</td>\n",
              "      <td>0</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>Мукополисахаридозы</td>\n",
              "      <td>Мукополисахаридозы</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9046</th>\n",
              "      <td>666</td>\n",
              "      <td>1</td>\n",
              "      <td>PLAIN</td>\n",
              "      <td>типов</td>\n",
              "      <td>типов</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9047</th>\n",
              "      <td>666</td>\n",
              "      <td>2</td>\n",
              "      <td>CARDINAL</td>\n",
              "      <td>III</td>\n",
              "      <td>три</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9048</th>\n",
              "      <td>666</td>\n",
              "      <td>3</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9049</th>\n",
              "      <td>666</td>\n",
              "      <td>4</td>\n",
              "      <td>LETTERS</td>\n",
              "      <td>IV</td>\n",
              "      <td>i v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9050</th>\n",
              "      <td>666</td>\n",
              "      <td>5</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9051</th>\n",
              "      <td>666</td>\n",
              "      <td>6</td>\n",
              "      <td>LETTERS</td>\n",
              "      <td>VI</td>\n",
              "      <td>v i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9052</th>\n",
              "      <td>666</td>\n",
              "      <td>7</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9053</th>\n",
              "      <td>666</td>\n",
              "      <td>8</td>\n",
              "      <td>CARDINAL</td>\n",
              "      <td>VII</td>\n",
              "      <td>семь</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9054</th>\n",
              "      <td>666</td>\n",
              "      <td>9</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentence_id  token_id     class              before               after\n",
              "9045          666         0     PLAIN  Мукополисахаридозы  Мукополисахаридозы\n",
              "9046          666         1     PLAIN               типов               типов\n",
              "9047          666         2  CARDINAL                 III                 три\n",
              "9048          666         3     PUNCT                   ,                   ,\n",
              "9049          666         4   LETTERS                  IV                 i v\n",
              "9050          666         5     PUNCT                   ,                   ,\n",
              "9051          666         6   LETTERS                  VI                 v i\n",
              "9052          666         7     PUNCT                   ,                   ,\n",
              "9053          666         8  CARDINAL                 VII                семь\n",
              "9054          666         9     PUNCT                   .                   ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvT164t3cq2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6896be74-d899-4f98-935b-b85d683f6beb"
      },
      "source": [
        "set(data_[data_['before'] == 'IV']['class'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LETTERS', 'ORDINAL'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emQGJ-I1cepJ"
      },
      "source": [
        "> Ну тут даже в исходном датасете не все римские числа были корректно типизированы. Это может сказаться на нашей нейронке. Ладно, продолжаем. Хотя, вроде IV -> 'четыре' нет. Там только порядковые как число определяются."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJGwExRxNMRI"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb = torch.nn.Embedding(len(data.idx2word), embedding_dim)\n",
        "        self.rnn = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, dropout=0.6)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.emb(x).view(-1, 1, self.embedding_dim) # (seq_len = -1, batch = 1, input_size = embeddin_dim)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMhYYroOJavv"
      },
      "source": [
        "> Смотрим, как должен выглядить Decoder с Attention. Самый простой:\n",
        "\n",
        "1.   $v_i = embedding(token_i)$\n",
        "2.   $\\alpha^T_i = v_i\\cdot W\\cdot output_E$\n",
        "3.   $att_i = output_E\\cdot softmax(\\alpha_i)$\n",
        "4.   $hidden_{D, i} = GRU(v_i, att_i, hidden_{D, i - 1})$\n",
        "5.   $Probas_i = softmax(FC\\cdot hidden_{D, i})$\n",
        "\n",
        "Потом можем добавить ещё одну RNN для подсчёта v_i."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAp2MatNYkC7"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb = torch.nn.Embedding(len(data.idx2word), embedding_dim)\n",
        "        self.rnn = torch.nn.GRU(input_size=embedding_dim + hidden_size, hidden_size=hidden_size, dropout=0.6)\n",
        "        self.W = torch.nn.Linear(embedding_dim, hidden_size)\n",
        "        self.FC = torch.nn.Linear(hidden_size, len(data.idx2word))\n",
        "\n",
        "    def forward(self, token, hidden, enc_out): # here goes only one token\n",
        "        # token : (1, 1, 1)\n",
        "        # hidden : (1, 1, hidden_size)\n",
        "        # enc_out : (1, hidden_size, enc_input)\n",
        "        embedded = self.emb(token).view(1, 1, self.embedding_dim) # v_i\n",
        "        att_w = F.softmax(torch.transpose(torch.bmm(self.W(embedded), enc_out), 1, 2), dim=1) # (batch, enc_seq_out, 1)\n",
        "        att = torch.bmm(enc_out, att_w).view(1, 1, -1) # (1, 1, hidden)\n",
        "        x = torch.cat((att, embedded), dim=2) # (1, 1, hidden + embedding)\n",
        "        output, hidden = self.rnn(F.silu(x), hidden)\n",
        "        probas = self.FC(hidden)\n",
        "        return probas, hidden, att_w\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyFV9MjvynW_"
      },
      "source": [
        "> теперь обучение...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn6BZSXvYkAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b420ff73-ac7c-4828-d2a1-cd0803ae5869"
      },
      "source": [
        "encoder = EncoderRNN(128, 128).to(device)\n",
        "decoder = DecoderRNN(128, 128).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3oNP45uYj-K"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=0.0004)\n",
        "optimizer_D = torch.optim.Adam(decoder.parameters(), lr=0.0004)\n",
        "EPOCHS = 50 # 60 epochs is +- enough\n",
        "teacher_forcing_ratio = 0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyAnywRBCcct"
      },
      "source": [
        "> Я запустил обучение на всём датасете и внезапно понял, что он огромный. Очень. Ожидаемое время одной эпохи - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyjOJu3PMezH",
        "outputId": "9fbc736f-7747-433c-a5f1-3575c79252ed"
      },
      "source": [
        "len(data.idx2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7342"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9-IGFYYYj74",
        "outputId": "f8775300-3482-4571-d090-d9c97f36f4f9"
      },
      "source": [
        "hist_losses = []\n",
        "end_token_id = data.word2idx['<END>']\n",
        "for epoch in range(EPOCHS):\n",
        "    permutation = np.random.permutation(len(data))\n",
        "    true_target = []\n",
        "    predicted_target = []\n",
        "    for j in range(len(data)):\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "        X, y = data[permutation[j]]\n",
        "\n",
        "        # Take random prefix\n",
        "        sz = len(X)\n",
        "        r_bound = random.randint(max(2, sz // 2), sz - 1) + 1\n",
        "        X = X[:r_bound] + [data.word2idx['<END>']]\n",
        "        y = y[:r_bound] + [data.word2idx['<END>']]\n",
        "\n",
        "        X = torch.tensor(X, device=device).view(1, -1)\n",
        "        # y = torch.tensor(y, device=device).view(1, -1)\n",
        "        hidden = encoder.initHidden()\n",
        "        output_E, hidden = encoder(X, hidden)\n",
        "        # output_E : seq x 1 x hidden_size <-- need reshape\n",
        "        # hidden : 1 x 1 x hidden_size\n",
        "        output_E = torch.swapaxes(output_E, 0, 1)\n",
        "        output_E = torch.swapaxes(output_E, 1, 2)\n",
        "        # output_E : 1 x hidden_size x seq\n",
        "        loss = 0\n",
        "        true_target.append('')\n",
        "        predicted_target.append('')\n",
        "        # print(len(y) - 1)\n",
        "        \n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "        if use_teacher_forcing:\n",
        "            for i in range(len(y) - 1):\n",
        "                token = torch.tensor([y[i]], device=device)\n",
        "                token = token.view(1, 1, 1)\n",
        "\n",
        "                probas, hidden, att_w = decoder(token, hidden, output_E)\n",
        "                # print(probas.view(1, -1).shape)\n",
        "                loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "                true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "                idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "                predicted_target[-1] += ' ' + data.idx2word[idx_pred]\n",
        "        else:\n",
        "            token = torch.tensor([y[0]], device=device) \n",
        "            for i in range(len(y) - 1):\n",
        "                token = token.to(device).view(1, 1, 1)\n",
        "\n",
        "                probas, hidden, att_w = decoder(token, hidden, output_E)\n",
        "                # print(probas.view(1, -1).shape)\n",
        "                loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "                true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "                idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "                predicted_target[-1] += ' ' + data.idx2word[idx_pred]\n",
        "                token = torch.tensor([idx_pred], device=device)\n",
        "                if idx_pred == end_token_id:\n",
        "                    break\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "        optimizer_E.step()\n",
        "        hist_losses.append(loss.detach().cpu().item() / (len(y) - 1))\n",
        "        if (j % 210 == 0):\n",
        "            print(j, np.mean(hist_losses[max(0, j - 42):-1]))\n",
        "    print(f'Epoch {epoch} finished, loss : {hist_losses[-1]:.7f}')\n",
        "    for i in range(1, 2):\n",
        "        print(true_target[-i], predicted_target[-i])\n",
        "    print('================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 nan\n",
            "210 6.180996584895361\n",
            "420 6.169732103889494\n",
            "630 6.590951806450275\n",
            "840 5.787517022044737\n",
            "Epoch 0 finished, loss : 3.4263179\n",
            " начальник цос фсб рф андрей ларюшин  в в , , . <END>\n",
            "================\n",
            "0 6.095761981802\n",
            "210 6.129911499687391\n",
            "420 6.097486532490127\n",
            "630 6.071718017836658\n",
            "840 6.037387165868788\n",
            "Epoch 1 finished, loss : 3.9215999\n",
            " энциклопедия туризма кирилла и  проверено ( . <END>\n",
            "================\n",
            "0 6.067619482991958\n",
            "210 6.045388059644612\n",
            "420 6.036047439020831\n",
            "630 5.990507836004399\n",
            "840 5.930082722396339\n",
            "Epoch 2 finished, loss : 4.3863440\n",
            " съемок ) « небо зовет  проверено « » » <END>\n",
            "================\n",
            "0 5.970414654227882\n",
            "210 5.9411951807606345\n",
            "420 5.926149331416791\n",
            "630 5.877812552689321\n",
            "840 5.834530932202376\n",
            "Epoch 3 finished, loss : 5.9016044\n",
            " в четвертую неделю альбом покинул топ - десять , опустившись <END>  в конце , , , , , , , , ,\n",
            "================\n",
            "0 5.881423084228117\n",
            "210 5.8534846097007\n",
            "420 5.82308854381745\n",
            "630 5.778858404049257\n",
            "840 5.728103849541764\n",
            "Epoch 4 finished, loss : 6.6900155\n",
            " альбом получил платиновый статус riaa и в целом положительные отзывы музыкальных критиков и интернет - изданий <END>  на в в на на на на на и и в <END> и <END> <END> <END> <END>\n",
            "================\n",
            "0 5.776653629246798\n",
            "210 5.743305374695434\n",
            "420 5.711690752111908\n",
            "630 5.667810589576324\n",
            "840 5.626440750075734\n",
            "Epoch 5 finished, loss : 6.1660102\n",
            " фудзимото вышел на поле на восемьдесят пятый минуте , заменив сюнсукэ накамуру . <END> <END>  после с на и и , , , , и , <END> <END> <END>\n",
            "================\n",
            "0 5.677811572551831\n",
            "210 5.6434300751170525\n",
            "420 5.610593273546638\n",
            "630 5.56382192612391\n",
            "840 5.51952459038544\n",
            "Epoch 6 finished, loss : 3.4312064\n",
            " — м. : воениздат , тысяча девятьсот восемьдесят семь . — т. один / <END>  — м. : , , . — — — — — .\n",
            "================\n",
            "0 5.567173598361422\n",
            "210 5.533264733125853\n",
            "420 5.497806016793176\n",
            "630 5.452813908317899\n",
            "840 5.410143157784506\n",
            "Epoch 7 finished, loss : 5.5949135\n",
            " кроме e - mail в уатс можно интегрировать <END>  кроме - в в в в в в в\n",
            "================\n",
            "0 5.462562558791765\n",
            "210 5.428706368176906\n",
            "420 5.391024630422544\n",
            "630 5.347113019289956\n",
            "840 5.304802417427101\n",
            "Epoch 8 finished, loss : 4.3626089\n",
            " восемьдесят семь тысяч четыреста сорок два , донецька область , першотравневий р - н , с <END>  кроме , , , , , , , <END> - <END> <END>\n",
            "================\n",
            "0 5.357743463089282\n",
            "210 5.321633067171682\n",
            "420 5.284944049386801\n",
            "630 5.239460865826538\n",
            "840 5.199446080194452\n",
            "Epoch 9 finished, loss : 4.1633785\n",
            " в антропологии термин « первый контакт » ( англ . first contact ) используется для описания первой встречи двух <END>  в составе « « » » » ( ( . ) ( ) <END> <END> <END> <END> <END> <END> <END>\n",
            "================\n",
            "0 5.254579830409657\n",
            "210 5.218603008632395\n",
            "420 5.180460449477838\n",
            "630 5.138396817436781\n",
            "840 5.096604437410235\n",
            "Epoch 10 finished, loss : 2.7159516\n",
            " паранин виктор иванович . <END> <END>  окончил <END> . . <END> <END>\n",
            "================\n",
            "0 5.150973091620856\n",
            "210 5.116906717609418\n",
            "420 5.082218587061711\n",
            "630 5.042987606496287\n",
            "840 4.999758543082639\n",
            "Epoch 11 finished, loss : 5.2683897\n",
            " седьмого октября две тысячи десятого года территория республики конго отошла к новоучрежденной браззавильской и габонской епархии <END>  бабочки была на и и и самолет и и <END> <END> <END>\n",
            "================\n",
            "0 5.0542432608063965\n",
            "210 5.019657345879304\n",
            "420 4.985116629635796\n",
            "630 4.943650025749768\n",
            "840 4.903687817991806\n",
            "Epoch 12 finished, loss : 2.5195453\n",
            " — тысяча пятьсот восемьдесят четыре ) дионисий ( упом . <END>  — ( — . ( англ . <END>\n",
            "================\n",
            "0 4.960038942727191\n",
            "210 4.926039630721081\n",
            "420 4.889188629317352\n",
            "630 4.846533139056468\n",
            "840 4.8082787998016965\n",
            "Epoch 13 finished, loss : 3.0001361\n",
            " три территории : нунавут , северо - западные <END>  самый - : - , а - <END> <END>\n",
            "================\n",
            "0 4.864045902968598\n",
            "210 4.831400049949839\n",
            "420 4.795662896488126\n",
            "630 4.757106896709227\n",
            "840 4.715971032680849\n",
            "Epoch 14 finished, loss : 4.4540990\n",
            " пленка хранится в паспорте агента , который похитили <END>  летом в , , , , в в <END>\n",
            "================\n",
            "0 4.771188254143303\n",
            "210 4.73793951333326\n",
            "420 4.701194892206151\n",
            "630 4.66262161751818\n",
            "840 4.622524023439638\n",
            "Epoch 15 finished, loss : 4.0267906\n",
            " всего за этот клуб он провел шестьдесят три матча <END>  всего в этот сельского и и <END> <END> <END>\n",
            "================\n",
            "0 4.6779676863922885\n",
            "210 4.64373950783642\n",
            "420 4.608039845273539\n",
            "630 4.569175556938775\n",
            "840 4.5298961732678675\n",
            "Epoch 16 finished, loss : 2.6597247\n",
            " проверено восемнадцатого июля две тысячи одиннадцатого года . jean shrimpton in melbourne <END>  проверено девятнадцатого февраля две тысячи шестнадцатого года . архивировано of <END> <END> <END>\n",
            "================\n",
            "0 4.585148232892135\n",
            "210 4.5520291377570015\n",
            "420 4.516384173931196\n",
            "630 4.47658826648996\n",
            "840 4.437697999818388\n",
            "Epoch 17 finished, loss : 3.6895358\n",
            " биография александра сергеевича лютого на сайте полтава историческая <END>  биография была сергеевича на на сайте сайте <END> <END>\n",
            "================\n",
            "0 4.4925398971388955\n",
            "210 4.459732015087265\n",
            "420 4.423643927437772\n",
            "630 4.385972888361327\n",
            "840 4.348075712558755\n",
            "Epoch 18 finished, loss : 2.0028894\n",
            " тысяча девятьсот шестьдесят один — « севильский цирюльник » дж . <END> <END>  тысяча девятьсот шестьдесят один — « севильский » — . <END> <END> <END>\n",
            "================\n",
            "0 4.402170162005323\n",
            "210 4.369323867948531\n",
            "420 4.334301022405357\n",
            "630 4.29415515355811\n",
            "840 4.256494860398747\n",
            "Epoch 19 finished, loss : 3.4074980\n",
            " прошел срочную военную службу в качестве сапера в пехотном полку .  турнир проводился в качестве в качестве в . . . <END>\n",
            "================\n",
            "0 4.312279829051366\n",
            "210 4.279107221041566\n",
            "420 4.2445056020014365\n",
            "630 4.206339167543167\n",
            "840 4.167872396924821\n",
            "Epoch 20 finished, loss : 1.8317745\n",
            " проверено четырнадцатого января две тысячи двенадцатого года . архивировано из первоисточника двадцать четвертого апреля две тысячи двенадцатого года . deepwater horizon <END>  проверено двадцать второго декабря две тысячи десятого года . архивировано из первоисточника четырнадцатого августа две тысячи одиннадцатого года . . численность населения\n",
            "================\n",
            "0 4.222345576579595\n",
            "210 4.188618557241379\n",
            "420 4.153063884802899\n",
            "630 4.114446171686084\n",
            "840 4.076768510558\n",
            "Epoch 21 finished, loss : 2.8828496\n",
            " нидерланды : ограниченная поставка из сша в тысяча девятьсот двадцать четвертом году , после второй мировой войны велось  нидерланды : ограниченная поставка в в , , в , , в , <END>\n",
            "================\n",
            "0 4.132335866991501\n",
            "210 4.099888297690182\n",
            "420 4.063574357456838\n",
            "630 4.025379213405491\n",
            "840 3.9872191292920283\n",
            "Epoch 22 finished, loss : 2.5521274\n",
            " в тысяча восемьсот девяносто пятом тысяча восемьсот девяносто шестом годах служил на черноморском флоте на канонерской лодке « терец » .  в тысяча восемьсот девяносто пятом тысяча восемьсот девяносто шестом на служил на черноморском флоте на всесоюзном на « привести на <END>\n",
            "================\n",
            "0 4.0420344068405045\n",
            "210 4.008503303424372\n",
            "420 3.973461944065585\n",
            "630 3.9357680411013836\n",
            "840 3.8975568747014786\n",
            "Epoch 23 finished, loss : 1.6087408\n",
            " psychiatric bulletin . тысяча девятьсот девяносто восемь ; двадцать два : от трехсот девятнадцати - трехсот двадцати colm cooney <END>  psychiatric bulletin : : : двадцать два : трехсот девятнадцати трехсот девятнадцати - трехсот двадцати colm <END> <END>\n",
            "================\n",
            "0 3.9523388251772795\n",
            "210 3.919713867805654\n",
            "420 3.883354490209341\n",
            "630 3.8455449553359493\n",
            "840 3.808945469184617\n",
            "Epoch 24 finished, loss : 2.3271405\n",
            " тысяча девятьсот шесть . ibis , p . сто тридцать три . on the breeding - habits of the rosy gull and the pectoral sandpiper <END>  спб . , , тысяча девятьсот восемьдесят девять . сто тридцать три . , государственный breeding - habits the the creation of and <END> <END> <END> .\n",
            "================\n",
            "0 3.863362524440715\n",
            "210 3.8313429826874263\n",
            "420 3.796527626532733\n",
            "630 3.759047074209993\n",
            "840 3.722553573111697\n",
            "Epoch 25 finished, loss : 2.1733826\n",
            " яз . , « руссо » , тысяча девятьсот девяносто четыре . — с. двадцать три . —  яз . , « мысль » , « руссо » , « . <END>\n",
            "================\n",
            "0 3.7768506401957516\n",
            "210 3.744818210258691\n",
            "420 3.709553290864331\n",
            "630 3.6718970211547575\n",
            "840 3.6348703787050543\n",
            "Epoch 26 finished, loss : 0.7147686\n",
            " член королевского общества канады ( тысяча девятьсот восемьдесят пять ) . <END> <END>  член королевского общества канады ( первого июня две тысячи десятого года ) . <END> <END>\n",
            "================\n",
            "0 3.689290487994511\n",
            "210 3.6577481982078077\n",
            "420 3.623185826626077\n",
            "630 3.5860839274611194\n",
            "840 3.5491615002507486\n",
            "Epoch 27 finished, loss : 3.4143944\n",
            " перед релизом альбома журнал billboard сообщал , что для продвижения пластинки организовывалась масштабная рекламная  летом кормится разнообразными журнал billboard сообщал , что для для поддержания на постройку <END>\n",
            "================\n",
            "0 3.6032269293110595\n",
            "210 3.5715968242094824\n",
            "420 3.537228568985509\n",
            "630 3.5008024868643437\n",
            "840 3.4642295834847796\n",
            "Epoch 28 finished, loss : 1.3637100\n",
            " ролики социальной рекламы ( восемьдесят ) , два <END>  вблизи социальной рекламы ( ( ) , , <END>\n",
            "================\n",
            "0 3.518882679026961\n",
            "210 3.488372154300737\n",
            "420 3.4543192161278977\n",
            "630 3.417886820005001\n",
            "840 3.3816021710209436\n",
            "Epoch 29 finished, loss : 0.5409207\n",
            " энциклопедия туризма кирилла и мефодия <END>  энциклопедия туризма кирилла и мефодия <END>\n",
            "================\n",
            "0 3.4355136580142873\n",
            "210 3.4052801818343656\n",
            "420 3.371460226637672\n",
            "630 3.3358863619074888\n",
            "840 3.301153533034926\n",
            "Epoch 30 finished, loss : 1.0150238\n",
            " проверено одиннадцатого июля две тысячи четырнадцатого года . тетяна пол <END>  проверено одиннадцатого июля две тысячи четырнадцатого года . тетяна пол <END>\n",
            "================\n",
            "0 3.3552821803300663\n",
            "210 3.325434400801241\n",
            "420 3.2927965487534068\n",
            "630 3.2578218374417727\n",
            "840 3.222565228166505\n",
            "Epoch 31 finished, loss : 0.5885802\n",
            " philippe два auguste ) захватывает графство анжу , а вместе с <END>  philippe два auguste ) захватывает графство анжу , а вместе с <END>\n",
            "================\n",
            "0 3.27687022792829\n",
            "210 3.248175321702653\n",
            "420 3.216482693637007\n",
            "630 3.1825169143509764\n",
            "840 3.1480421382504296\n",
            "Epoch 32 finished, loss : 0.7285401\n",
            " максим коваль ( рус <END>  максим коваль ( <END> <END>\n",
            "================\n",
            "0 3.202291944879953\n",
            "210 3.173866036350529\n",
            "420 3.141923616877651\n",
            "630 3.1079341253168673\n",
            "840 3.0744523348251818\n",
            "Epoch 33 finished, loss : 0.5517177\n",
            " по данным переписи две тысячи девятого года , в селе проживало девять тысяч пятьсот семьдесят пять человек ( четыре тысячи пятьсот восемнадцать <END>  по данным переписи две тысячи девятого года , в селе проживало девять тысяч пятьсот семьдесят пять человек ( четыре тысячи пятьсот восемнадцать мужчин\n",
            "================\n",
            "0 3.1289607993817095\n",
            "210 3.101097802467926\n",
            "420 3.069911753206707\n",
            "630 3.0369684312003655\n",
            "840 3.0036199301123423\n",
            "Epoch 34 finished, loss : 0.2542039\n",
            " главным достижением клуба является победа в национальном чемпионате в <END>  главным достижением клуба является победа в национальном чемпионате в <END>\n",
            "================\n",
            "0 3.0583140070411243\n",
            "210 3.0309636703348652\n",
            "420 3.0003212672907225\n",
            "630 2.967770543521834\n",
            "840 2.935297379510202\n",
            "Epoch 35 finished, loss : 2.4551939\n",
            " проверено четырнадцатого января две тысячи двенадцатого года . архивировано из первоисточника двадцать четвертого апреля две тысячи двенадцатого года . deepwater horizon launched by tsf <END>  проверено пятого июля две тысячи четырнадцатого года . архивировано из первоисточника двадцать четвертого мая две тысячи тринадцатого года . top пятьдесят manga creators by sales\n",
            "================\n",
            "0 2.989897620926972\n",
            "210 2.963373752740851\n",
            "420 2.9331594855595835\n",
            "630 2.901232687444449\n",
            "840 2.8689381386475845\n",
            "Epoch 36 finished, loss : 0.5009139\n",
            " — ки ї в : ф і тосоц і оцентр , две тысячи четыре . <END>  — ки ї в : ф і тосоц і оцентр , две тысячи четыре . <END>\n",
            "================\n",
            "0 2.923304309075806\n",
            "210 2.8971162389461425\n",
            "420 2.8679490188945396\n",
            "630 2.8367524497316596\n",
            "840 2.805451121214865\n",
            "Epoch 37 finished, loss : 0.2597873\n",
            " new york : wiley <END>  new york : wiley <END>\n",
            "================\n",
            "0 2.8596859530739356\n",
            "210 2.83393252825769\n",
            "420 2.8050396365939054\n",
            "630 2.7744125711752043\n",
            "840 2.7437230483183463\n",
            "Epoch 38 finished, loss : 3.2647135\n",
            " — спб . : питер , две тысячи два . — с. от восьмидесяти одного — восемьдесят восемь <END>  — спб . , м. . — с. от восьмидесяти одного — восемьдесят восемь . — <END>\n",
            "================\n",
            "0 2.7979769893241735\n",
            "210 2.773044534773066\n",
            "420 2.744905176589946\n",
            "630 2.715005918387647\n",
            "840 2.685040493171755\n",
            "Epoch 39 finished, loss : 0.1685030\n",
            " мумбаи расположен в устье реки улхас ( англ . ulhas ) , <END>  мумбаи расположен в устье реки улхас ( англ . ulhas ) , <END>\n",
            "================\n",
            "0 2.7392547405118695\n",
            "210 2.7145097569867596\n",
            "420 2.686965187702499\n",
            "630 2.6574910214936662\n",
            "840 2.6278518041262133\n",
            "Epoch 40 finished, loss : 0.2367925\n",
            " в трех тоннах — м. : т - во науч . изд . кмк , ин - <END>  в трех тоннах — м. : т - во науч . изд . кмк , ин - <END>\n",
            "================\n",
            "0 2.6819005525460193\n",
            "210 2.65776358336634\n",
            "420 2.6306021284104357\n",
            "630 2.6018510840306406\n",
            "840 2.572756834544369\n",
            "Epoch 41 finished, loss : 0.4158033\n",
            " — м. : пко « картография » : изд - во оникс , две тысячи десять . <END>  — м. : пко « картография » : изд - во оникс , две тысячи десять . <END>\n",
            "================\n",
            "0 2.6264108337427308\n",
            "210 2.602753566831465\n",
            "420 2.5762149329438206\n",
            "630 2.5479004061764243\n",
            "840 2.519494916726426\n",
            "Epoch 42 finished, loss : 0.1653924\n",
            " здесь p является инвариантом цикла <END>  здесь p является инвариантом цикла <END>\n",
            "================\n",
            "0 2.5729728316228506\n",
            "210 2.5499638710424777\n",
            "420 2.524161563166365\n",
            "630 2.4966668223838395\n",
            "840 2.4689245813297096\n",
            "Epoch 43 finished, loss : 0.5712029\n",
            " в районе лепель наши войска , упорно обороняя каждый рубеж , сдерживают наступление крупных мотомеханизированных частей противника . <END> <END>  в районе лепель наши войска , упорно обороняя каждый рубеж , сдерживают наступление крупных мотомеханизированных частей . <END> <END> <END>\n",
            "================\n",
            "0 2.5223365436029366\n",
            "210 2.4997624641635827\n",
            "420 2.4743255934271766\n",
            "630 2.4471358121481335\n",
            "840 2.4198096320283877\n",
            "Epoch 44 finished, loss : 0.2521891\n",
            " главным достижением клуба является победа в национальном чемпионате в тысяча девятьсот семьдесят девятом году . <END>  главным достижением клуба является победа в национальном чемпионате в тысяча девятьсот семьдесят девятом году . <END>\n",
            "================\n",
            "0 2.473192189136975\n",
            "210 2.4512034322095455\n",
            "420 2.426295011765483\n",
            "630 2.399670118995609\n",
            "840 2.372862360171359\n",
            "Epoch 45 finished, loss : 0.1013289\n",
            " получение информации об адресах , почтовых индексах , странах <END>  получение информации об адресах , почтовых индексах , странах <END>\n",
            "================\n",
            "0 2.426064304756067\n",
            "210 2.404522960861266\n",
            "420 2.37986797592287\n",
            "630 2.353799420432189\n",
            "840 2.3275618454196856\n",
            "Epoch 46 finished, loss : 0.2229410\n",
            " cnn ( шесть january две тысячи четырнадцать ) . <END>  cnn ( шесть january две тысячи четырнадцать ) . <END>\n",
            "================\n",
            "0 2.3800930671350637\n",
            "210 2.359002054901055\n",
            "420 2.335155229453897\n",
            "630 2.3096753516930377\n",
            "840 2.2838465181247147\n",
            "Epoch 47 finished, loss : 0.0518321\n",
            " пант - лауреат премии имени джавахарлала <END>  пант - лауреат премии имени джавахарлала <END>\n",
            "================\n",
            "0 2.3361886374197054\n",
            "210 2.3154443622840386\n",
            "420 2.2918587968836834\n",
            "630 2.266619500866624\n",
            "840 2.241411963695941\n",
            "Epoch 48 finished, loss : 0.0755078\n",
            " также « провинция » оказывает полный <END>  также « провинция » оказывает полный <END>\n",
            "================\n",
            "0 2.293436223819705\n",
            "210 2.2731678729153697\n",
            "420 2.2499881397861863\n",
            "630 2.2253063768600576\n",
            "840 2.200515674304671\n",
            "Epoch 49 finished, loss : 0.3340515\n",
            " далай - лама и многие министры были вынуждены бежать в индию , где им было предоставлено прибежище <END>  далай - лама и многие министры были вынуждены бежать в индию , где им было предоставлено <END> <END>\n",
            "================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pXtwIRDKPva"
      },
      "source": [
        "Пока оно обучается: небольшой отчёт.\n",
        "\n",
        "Было тяжело. Поэтому я поставил много костылей.\n",
        "\n",
        "> Изначально в датасете около 700k предложений. Ожидаемое время обучения было около 60-80 **часов** на эпоху. Поэтому я взял меньше предложений. Но оказалось, что там много различных слов и нейронка должна очень долго учиться, чтобы не ставить много точек везде. Поэтому я оставил всего 840 предложений ~ 6k различных токенов (слов).\n",
        "\n",
        "> Далее из-за небольшого словаря он не смог распознать слово 'Эти'. Именно с большой буквы. Я уверен, что аналог с маленькой буквы есть в словаре. Поэтому я принял решение убрать заглавные буквы. Так станет чуть-чуть проще. Ну и после всех этих преобразований нейросеть неплохо обучается за 50 эпох (судя по тренировочной выборке)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvZ7E_cvYj5M",
        "outputId": "62944057-b9a9-4724-bb35-2532f8726644"
      },
      "source": [
        "for X, y in data:\n",
        "    print(data.detokenize(y))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<START>', 'по', 'состоянию', 'на', 'тысяча восемьсот шестьдесят второй год', '.', '<END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F43na8_EYjzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "17c25b28-15e6-4bca-8c08-410cfe4bcd17"
      },
      "source": [
        "plt.plot(hist_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9a3ad71750>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70UzLlARTU3w"
      },
      "source": [
        "def prepare_sent(sent):\n",
        "    seq = []\n",
        "    for word in sent:\n",
        "        if word not in data.idx2word:\n",
        "            word = '<UNK>'\n",
        "        seq.append(data.word2idx[word])\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMj7tk9oNMPS"
      },
      "source": [
        "def predict(seq):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    seq = prepare_sent(seq)\n",
        "    predicted_target = ''\n",
        "    att = []\n",
        "    with torch.no_grad():\n",
        "        X = seq\n",
        "        X = torch.tensor(X, device=device).view(1, -1)\n",
        "        # y = torch.tensor(y, device=device).view(1, -1)\n",
        "        hidden = encoder.initHidden()\n",
        "        output_E, hidden = encoder(X, hidden)\n",
        "        # output_E : seq x 1 x hidden_size <-- need reshape\n",
        "        # hidden : 1 x 1 x hidden_size\n",
        "        output_E = torch.swapaxes(output_E, 0, 1)\n",
        "        output_E = torch.swapaxes(output_E, 1, 2)\n",
        "        # output_E : 1 x hidden_size x seq\n",
        "        # true_target = ''\n",
        "        # print(len(y) - 1)\n",
        "        token = torch.tensor([data.word2idx['<START>']], device=device) \n",
        "        word_hidden = decoder.initHidden()\n",
        "        for i in range(228):\n",
        "            token = token.to(device).view(1, 1, 1)\n",
        "\n",
        "            probas, hidden, att_w, word_hidden = decoder(token, hidden, output_E, word_hidden)\n",
        "            att.append(torch.squeeze(att_w).detach().cpu().numpy())\n",
        "            # print(probas.view(1, -1).shape)\n",
        "            # loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "            # true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "            idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "            predicted_target += ' ' + data.idx2word[idx_pred]\n",
        "            token = torch.tensor([idx_pred], device=device)\n",
        "            if idx_pred == end_token_id:\n",
        "                break\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    return predicted_target, att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS967I2gNL47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6682f28c-6500-415b-d03a-7923ec8c4da0"
      },
      "source": [
        "id = 13\n",
        "# id = 51842\n",
        "print('Before : ', dataset[id])\n",
        "print('True : ', target[id])\n",
        "# predict(['<START>', 'в', 'конце', '1811 года', ',', 'вследствие', 'конфликта', '.', '<END>'])[0]\n",
        "predict(dataset[id])[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before :  ['<START>', 'революция', '1905 года', 'потерпела', 'поражение', '.', '<END>']\n",
            "True :  ['<START>', 'революция', 'тысяча девятьсот пятого года', 'потерпела', 'поражение', '.', '<END>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' революция тысяча девятьсот пятого года потерпела поражение . <END>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_TiDqDh0VlBu",
        "outputId": "44a882c0-6619-4132-d635-1e8a6c008468"
      },
      "source": [
        "' '.join(dataset[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<START> в конце 1811 года , вследствие конфликта с проезжим вельможей ( графом салтыковым ) вынужден был оставить службу по личному прошению . <END>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "PXnLzORdhBLh",
        "outputId": "5c99665b-b87c-4904-803a-9668a83758e7"
      },
      "source": [
        "output_words, attentions = predict(dataset[13])#['<START>', 'проверено', 'в', 'конце', '1811 года', '.', '<END>'])\n",
        "plt.matshow(attentions)\n",
        "print(output_words)\n",
        "plt.show()\n",
        "attentions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " революция тысяча девятьсот пятого года потерпела поражение . <END>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 336x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([2.7882480e-30, 1.0000000e+00, 8.3713606e-16, 6.0067743e-24,\n",
              "        1.3879586e-32, 2.4232195e-31, 3.0217044e-35], dtype=float32),\n",
              " array([1.5706958e-12, 1.3345511e-08, 2.2581339e-08, 1.2065402e-07,\n",
              "        9.1716915e-02, 4.0522394e-01, 5.0305903e-01], dtype=float32),\n",
              " array([5.8661309e-16, 1.7264250e-12, 1.8741617e-10, 3.1731736e-07,\n",
              "        2.9038319e-03, 9.9709392e-01, 1.8499721e-06], dtype=float32),\n",
              " array([1.8713336e-05, 3.9400604e-13, 4.7568178e-06, 1.0001550e-07,\n",
              "        1.8801174e-08, 9.9996591e-01, 1.0454131e-05], dtype=float32),\n",
              " array([8.6406942e-14, 3.0284224e-05, 3.8212600e-01, 3.0019996e-03,\n",
              "        6.1467117e-01, 2.8283646e-05, 1.4222569e-04], dtype=float32),\n",
              " array([7.1777349e-18, 2.7088068e-25, 9.0359859e-20, 1.2729144e-17,\n",
              "        1.8412320e-16, 1.0000000e+00, 8.4540601e-16], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XrDozU584gu"
      },
      "source": [
        "> А вот Attention как-то не очень работает. Сейчас попробую улучшение: Я буду в decoder перед всем делать не просто embedding слова, а пропускать это слово через ещё одну GRU и как representation vector слова буду брать hidden из этой GRU. Он как-то эти все предложения запоминает, но даже префиксы тренировочного датасета плохо предсказывает."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9hzclB2Bs8E"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb = torch.nn.Embedding(len(data.idx2word), embedding_dim)\n",
        "        self.word_rnn = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, dropout=0.6)\n",
        "        self.rnn = torch.nn.GRU(input_size=embedding_dim + hidden_size, hidden_size=hidden_size, dropout=0.6)\n",
        "        self.W = torch.nn.Linear(embedding_dim, hidden_size)\n",
        "        self.drop = torch.nn.Dropout(0.6)\n",
        "        self.FC = torch.nn.Linear(hidden_size, len(data.idx2word))\n",
        "\n",
        "    def forward(self, token, hidden, enc_out, word_hidden): # here goes only one token\n",
        "        # token : (1, 1, 1)\n",
        "        # hidden : (1, 1, hidden_size)\n",
        "        # enc_out : (1, hidden_size, enc_input)\n",
        "        embedded = self.emb(token).view(1, 1, self.embedding_dim) # v_i\n",
        "        out, word = self.word_rnn(embedded, word_hidden)\n",
        "        att_w = F.softmax(torch.transpose(torch.bmm(self.drop(self.W(word)), enc_out), 1, 2), dim=1) # (batch, enc_seq_out, 1)\n",
        "        att = torch.bmm(enc_out, att_w).view(1, 1, -1) # (1, 1, hidden)\n",
        "        x = torch.cat((att, word), dim=2) # (1, 1, hidden + embedding)\n",
        "        output, hidden = self.rnn(F.silu(x), hidden)\n",
        "        probas = self.FC(hidden)\n",
        "        return probas, hidden, att_w, word\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vLR5aDIDbfX",
        "outputId": "81f2177d-d313-48f7-94b7-389431bee466"
      },
      "source": [
        "encoder = EncoderRNN(128, 128).to(device)\n",
        "decoder = DecoderRNN(128, 128).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Ownte1Dbb_"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
        "optimizer_D = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
        "EPOCHS = 50 # 60 epochs is +- enough\n",
        "teacher_forcing_ratio = 0.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHOTeeD4Bs5y",
        "outputId": "9684f141-f4d5-4f52-8216-6baab8e91357"
      },
      "source": [
        "hist_losses = []\n",
        "end_token_id = data.word2idx['<END>']\n",
        "for epoch in range(EPOCHS):\n",
        "    permutation = np.random.permutation(len(data))\n",
        "    true_target = []\n",
        "    predicted_target = []\n",
        "    for j in range(len(data)):\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "        X, y = data[permutation[j]]\n",
        "\n",
        "        # Take random segment\n",
        "        sz = len(X)\n",
        "        r_bound = random.randint(max(2, sz // 2), sz - 1) + 1\n",
        "        l_bound = random.randint(1, r_bound - 1)\n",
        "        X = [data.word2idx['<START>']] + X[l_bound:r_bound] + [data.word2idx['<END>']]\n",
        "        y = [data.word2idx['<START>']] + y[l_bound:r_bound] + [data.word2idx['<END>']]\n",
        "\n",
        "\n",
        "        X = torch.tensor(X, device=device).view(1, -1)\n",
        "        # y = torch.tensor(y, device=device).view(1, -1)\n",
        "        hidden = encoder.initHidden()\n",
        "        output_E, hidden = encoder(X, hidden)\n",
        "        # output_E : seq x 1 x hidden_size <-- need reshape\n",
        "        # hidden : 1 x 1 x hidden_size\n",
        "        output_E = torch.swapaxes(output_E, 0, 1)\n",
        "        output_E = torch.swapaxes(output_E, 1, 2)\n",
        "        # output_E : 1 x hidden_size x seq\n",
        "        loss = 0\n",
        "        true_target.append('')\n",
        "        predicted_target.append('')\n",
        "        # print(len(y) - 1)\n",
        "        word_hidden = decoder.initHidden()\n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "        if use_teacher_forcing:\n",
        "            for i in range(len(y) - 1):\n",
        "                token = torch.tensor([y[i]], device=device)\n",
        "                token = token.view(1, 1, 1)\n",
        "\n",
        "                probas, hidden, att_w, word_hidden = decoder(token, hidden, output_E, word_hidden)\n",
        "                # print(probas.view(1, -1).shape)\n",
        "                loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "                true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "                idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "                predicted_target[-1] += ' ' + data.idx2word[idx_pred]\n",
        "        else:\n",
        "            token = torch.tensor([y[0]], device=device) \n",
        "            for i in range(len(y) - 1):\n",
        "                token = token.to(device).view(1, 1, 1)\n",
        "\n",
        "                probas, hidden, att_w, word_hidden = decoder(token, hidden, output_E, word_hidden)\n",
        "                # print(probas.view(1, -1).shape)\n",
        "                loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "                true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "                idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "                predicted_target[-1] += ' ' + data.idx2word[idx_pred]\n",
        "                token = torch.tensor([idx_pred], device=device)\n",
        "                if idx_pred == end_token_id:\n",
        "                    break\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "        optimizer_E.step()\n",
        "        hist_losses.append(loss.detach().cpu().item() / (len(y) - 1))\n",
        "        if (j % 210 == 0):\n",
        "            print(j, np.mean(hist_losses[max(0, j - 42):-1]))\n",
        "    print(f'Epoch {epoch} finished, loss : {hist_losses[-1]:.7f}')\n",
        "    for i in range(1, 2):\n",
        "        print(true_target[-i], predicted_target[-i])\n",
        "    print('----------------')\n",
        "    print(predict(['<START>', 'в', 'конце', '1811 года', ',', 'вследствие', 'конфликта', '.', '<END>'])[0])\n",
        "    print('================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 nan\n",
            "210 1.1675334607106305\n",
            "420 0.9328023842550844\n",
            "630 1.2483834042247492\n",
            "840 1.172076068900302\n",
            "Epoch 0 finished, loss : 1.0251495\n",
            " административный центр поселения — хутор плотников <END>  хутор центр поселения — хутор плотников плотников\n",
            "----------------\n",
            " в крулл , , , деревне . <END>\n",
            "================\n",
            "0 1.0951203452519038\n",
            "210 1.1056973510795247\n",
            "420 1.1056060674898092\n",
            "630 1.1240375352497818\n",
            "840 1.12682816830217\n",
            "Epoch 1 finished, loss : 0.0198919\n",
            " сельского поселения . <END> <END>  сельского поселения . <END> <END>\n",
            "----------------\n",
            " в мастерской , , . . . <END>\n",
            "================\n",
            "0 1.1094656159147782\n",
            "210 1.1069080105047253\n",
            "420 1.115188500882707\n",
            "630 1.113210127883522\n",
            "840 1.1025331366415418\n",
            "Epoch 2 finished, loss : 0.9138207\n",
            " с запада , а одиннадцатая гвардейская атаковала <END>  с запада , а гвардейская гвардейская <END> <END>\n",
            "----------------\n",
            " в конце мастерской , . . <END>\n",
            "================\n",
            "0 1.1064198114423631\n",
            "210 1.1094144677038487\n",
            "420 1.1100994109738076\n",
            "630 1.1143981707129669\n",
            "840 1.1060751241789397\n",
            "Epoch 3 finished, loss : 0.1089230\n",
            " кандидат философских наук . <END> <END>  кандидат философских наук . <END> <END>\n",
            "----------------\n",
            " в конце входят , . . . <END>\n",
            "================\n",
            "0 1.099198450125631\n",
            "210 1.0993449984851207\n",
            "420 1.0932197194613313\n",
            "630 1.0944350385678612\n",
            "840 1.094549467773773\n",
            "Epoch 4 finished, loss : 0.5878221\n",
            " янусу или сатурну . <END>  янусу или сатурну . <END>\n",
            "----------------\n",
            " в конце человек , тысяча девятьсот девяносто семь . . <END>\n",
            "================\n",
            "0 1.094219977695829\n",
            "210 1.0947790505406605\n",
            "420 1.0985160796647087\n",
            "630 1.0955668598991193\n",
            "840 1.097119122753324\n",
            "Epoch 5 finished, loss : 1.1675646\n",
            " гидрометеоиздат , тысяча девятьсот шестьдесят четыре . — четыреста тридцать две секунды государственный водный реестр рф :  гидрометеоиздат , тысяча девятьсот шестьдесят четыре государственный — четыреста тридцать две секунды государственный государственный рф государственный <END>\n",
            "----------------\n",
            " в деревне составляет , , тысяча девятьсот девяносто семь . <END>\n",
            "================\n",
            "0 1.0966269083905877\n",
            "210 1.0960650996018437\n",
            "420 1.0943672877701678\n",
            "630 1.0914525013806344\n",
            "840 1.0875331375998813\n",
            "Epoch 6 finished, loss : 0.3686771\n",
            " синдрома при ультразвуковом  синдрома при <END>\n",
            "----------------\n",
            " в разных пестика , , . . <END>\n",
            "================\n",
            "0 1.0914782504475746\n",
            "210 1.0872865517083612\n",
            "420 1.0871062182823643\n",
            "630 1.0880008125498628\n",
            "840 1.0875914836678082\n",
            "Epoch 7 finished, loss : 3.1564159\n",
            " простые <END>  ссср <END>\n",
            "----------------\n",
            " в конце тысяча восемьсот одиннадцатого года , , . . <END>\n",
            "================\n",
            "0 1.0879644222676068\n",
            "210 1.0861436272335931\n",
            "420 1.0840304328444141\n",
            "630 1.084320888314714\n",
            "840 1.0838087621038843\n",
            "Epoch 8 finished, loss : 2.0477023\n",
            " медной монеты и появились при екатерине второй первые бумажные деньги — <END>  медной монеты и появились первые появились первые первые первые первые первые <END>\n",
            "----------------\n",
            " в конце входят , , которого . <END>\n",
            "================\n",
            "0 1.0844956685869123\n",
            "210 1.0856011445426028\n",
            "420 1.086349237892861\n",
            "630 1.0846646348683973\n",
            "840 1.0807976980599043\n",
            "Epoch 9 finished, loss : 2.7914360\n",
            " medecine des enfants , тысяча девятьсот тридцать три ; тридцать шесть : от семьсот тринадцать — семьсот девятнадцать schrier s. a. et al  тридцать шесть des enfants , тысяча девятьсот тридцать три — — от семьсот девятнадцать семьсот девятнадцать семьсот девятнадцать семьсот девятнадцать семьсот девятнадцать семьсот девятнадцать семьсот девятнадцать <END>\n",
            "----------------\n",
            " в конце составляет , , деревне . <END>\n",
            "================\n",
            "0 1.0817223574516555\n",
            "210 1.0767147331003741\n",
            "420 1.072066706650009\n",
            "630 1.0712317507865177\n",
            "840 1.068727592795903\n",
            "Epoch 10 finished, loss : 1.7152308\n",
            " устроил засаду <END>  засаду засаду <END>\n",
            "----------------\n",
            " в конце составляет , , . . <END>\n",
            "================\n",
            "0 1.0688061266862785\n",
            "210 1.0672985985295342\n",
            "420 1.0665558068358603\n",
            "630 1.0636405502042021\n",
            "840 1.061294974089445\n",
            "Epoch 11 finished, loss : 0.2855943\n",
            " из которых водная <END>  из которых водная <END>\n",
            "----------------\n",
            " в конце составляет , , . ле . <END>\n",
            "================\n",
            "0 1.061650561794772\n",
            "210 1.0599347099161227\n",
            "420 1.058531365564177\n",
            "630 1.0560631422727162\n",
            "840 1.053116372430278\n",
            "Epoch 12 finished, loss : 0.0027905\n",
            " ) <END>  ) <END>\n",
            "----------------\n",
            " в конце человек , . . . <END>\n",
            "================\n",
            "0 1.0575011527407054\n",
            "210 1.0550430886873823\n",
            "420 1.0506827532095517\n",
            "630 1.048002292032799\n",
            "840 1.0453452905425837\n",
            "Epoch 13 finished, loss : 2.2021118\n",
            " — спб . , две тысячи четырнадцать . печатное собрание лермонтовского музея при  — спб . , , , две тысячи четырнадцать . — с. <END>\n",
            "----------------\n",
            " в ткани , , , . . <END>\n",
            "================\n",
            "0 1.0473386436625725\n",
            "210 1.0450095121129208\n",
            "420 1.0434799629188165\n",
            "630 1.042751668458231\n",
            "840 1.0399239910288873\n",
            "Epoch 14 finished, loss : 0.1533050\n",
            " двусторонний . <END>  двусторонний . <END>\n",
            "----------------\n",
            " в конце , , , . <END>\n",
            "================\n",
            "0 1.0408859842150284\n",
            "210 1.0396878510266805\n",
            "420 1.035318228771162\n",
            "630 1.033041303228098\n",
            "840 1.030029918715999\n",
            "Epoch 15 finished, loss : 1.2798946\n",
            " . — двести шестьдесят две секунды результаты опроса восемьсот сорок шесть <END>  . — двести шестьдесят две секунды результаты опроса <END> <END>\n",
            "----------------\n",
            " в конце тысяча восемьсот одиннадцатого года , , . . <END>\n",
            "================\n",
            "0 1.0321521330664942\n",
            "210 1.0294070703452938\n",
            "420 1.0286346546086464\n",
            "630 1.0250470907852125\n",
            "840 1.0227473760586818\n",
            "Epoch 16 finished, loss : 0.0417579\n",
            " » , тысяча девятьсот девяносто четыре . <END>  » , тысяча девятьсот девяносто четыре . <END>\n",
            "----------------\n",
            " в конце , , когда . . <END>\n",
            "================\n",
            "0 1.0251404395809145\n",
            "210 1.021962190889928\n",
            "420 1.0199595956909415\n",
            "630 1.0173841814761466\n",
            "840 1.0150693153778734\n",
            "Epoch 17 finished, loss : 1.9461597\n",
            " в настоящий момент он занимает третье место среди лучших бомбардиров « <END>  в настоящий момент место занимает среди лучших лучших среди среди « бананца\n",
            "----------------\n",
            " в конце , , тысяча девятьсот девяносто семь . <END>\n",
            "================\n",
            "0 1.017659403305771\n",
            "210 1.0165062340777875\n",
            "420 1.014187683154087\n",
            "630 1.0129435620873881\n",
            "840 1.0101721290468029\n",
            "Epoch 18 finished, loss : 0.8872522\n",
            " районный центр — город арск был основан в средневековье как <END>  районный центр — арск арск был основан в средневековье <END> <END>\n",
            "----------------\n",
            " в конце ста двадцати шести , тысяча девятьсот девяносто семь . . <END>\n",
            "================\n",
            "0 1.013132302341304\n",
            "210 1.0109104286979764\n",
            "420 1.0088500234948126\n",
            "630 1.0065202410273122\n",
            "840 1.0040318504472685\n",
            "Epoch 19 finished, loss : 0.3721660\n",
            " сто шестьдесят четыре . masataka , n. ( <END>  сто шестьдесят четыре . masataka , n. <END> <END>\n",
            "----------------\n",
            " в конце , , когда . . <END>\n",
            "================\n",
            "0 1.0055172664491905\n",
            "210 1.004729479655488\n",
            "420 1.0005400598429914\n",
            "630 0.9981586559298947\n",
            "840 0.9952112998671122\n",
            "Epoch 20 finished, loss : 1.6214962\n",
            " простой у муравьедов или <END>  у у муравьедов или <END>\n",
            "----------------\n",
            " в конце , , , ее . . <END>\n",
            "================\n",
            "0 0.997840940444752\n",
            "210 0.9961314424495304\n",
            "420 0.9930690809037657\n",
            "630 0.9918798395393162\n",
            "840 0.9888219964536603\n",
            "Epoch 21 finished, loss : 0.9937156\n",
            " еврипид <END>  еврипид <END>\n",
            "----------------\n",
            " в конце тысяча девятьсот двадцать четвертом году , , . . <END>\n",
            "================\n",
            "0 0.991879407553824\n",
            "210 0.9894819193863018\n",
            "420 0.986892801348326\n",
            "630 0.9846199096175248\n",
            "840 0.9821963691756813\n",
            "Epoch 22 finished, loss : 0.7623056\n",
            " - , реже тонкомясистая , с <END>  - , реже реже с с <END>\n",
            "----------------\n",
            " в конце агента , , с . . <END>\n",
            "================\n",
            "0 0.9852680203632327\n",
            "210 0.9829808711752421\n",
            "420 0.9800447428916388\n",
            "630 0.9790944555807776\n",
            "840 0.9765302386323348\n",
            "Epoch 23 finished, loss : 1.0109215\n",
            " бен попросил ричарда привести ему « <END>  привести попросил ричарда привести ему « человека\n",
            "----------------\n",
            " в ста двадцати шести ; , , . . <END>\n",
            "================\n",
            "0 0.9795879983941844\n",
            "210 0.9782407396639956\n",
            "420 0.9760977894979329\n",
            "630 0.973461017079079\n",
            "840 0.9717194238613494\n",
            "Epoch 24 finished, loss : 2.9894969\n",
            " мио - инозитол ( i - инозит ; мезо - инозит ; мио - инозит ; 1,2 , 3,5 - цис - четыре , шести циклогексангексол ; дамбоза ) — один из девяти стереоизомеров шестиатомного спирта инозитола . <END>  мио - - ( - - - - - - - - , - четыре , , , , - — ) четыре ) 3,5 — ) <END> ) ) один из <END> один <END> один один <END> <END>\n",
            "----------------\n",
            " в в ста двадцати шести , , . . <END>\n",
            "================\n",
            "0 0.9757606745261337\n",
            "210 0.9747586697034543\n",
            "420 0.9723462186926626\n",
            "630 0.9703771261417244\n",
            "840 0.9675536835267816\n",
            "Epoch 25 finished, loss : 0.8920897\n",
            " нарушена , часто выявляется гипоурикемия . <END>  нарушена , часто часто гипоурикемия . <END>\n",
            "----------------\n",
            " в конце , , , . . <END>\n",
            "================\n",
            "0 0.971023262304509\n",
            "210 0.9700293998054634\n",
            "420 0.96722342822586\n",
            "630 0.9656309617934773\n",
            "840 0.9625322982239941\n",
            "Epoch 26 finished, loss : 0.8107922\n",
            " ирландии , <END>  ирландии , <END>\n",
            "----------------\n",
            " в конце , , . . . <END>\n",
            "================\n",
            "0 0.9663855907640947\n",
            "210 0.9648548387526581\n",
            "420 0.9634350884500518\n",
            "630 0.9607214863692438\n",
            "840 0.9592148583912613\n",
            "Epoch 27 finished, loss : 4.3051248\n",
            " волостей значительно сокращено  с. и. значительно <END>\n",
            "----------------\n",
            " в конце , , , , . <END>\n",
            "================\n",
            "0 0.9615569043350326\n",
            "210 0.9594183465227896\n",
            "420 0.9585400869627372\n",
            "630 0.9561412400741884\n",
            "840 0.9536744437073998\n",
            "Epoch 28 finished, loss : 0.3107318\n",
            " равен весовой унции , имея при <END>  равен весовой унции , имея при <END>\n",
            "----------------\n",
            " в конце ; , , , . . <END>\n",
            "================\n",
            "0 0.9561355725461748\n",
            "210 0.9544506295635988\n",
            "420 0.9520268680873454\n",
            "630 0.9504449928830012\n",
            "840 0.9488474043112599\n",
            "Epoch 29 finished, loss : 2.7600349\n",
            " семейное предприятие , членами которого стали его сыновья , дочь ,  дочь семнадцать , космический которого , его , , его <END>\n",
            "----------------\n",
            " в конце пять , , . <END>\n",
            "================\n",
            "0 0.9517793564335796\n",
            "210 0.9506004929007171\n",
            "420 0.9483207034810599\n",
            "630 0.946766672024695\n",
            "840 0.9448896026981001\n",
            "Epoch 30 finished, loss : 0.2245251\n",
            " его темой становятся испанские селения <END>  его темой становятся испанские селения <END>\n",
            "----------------\n",
            " в конце ; , . . . . <END>\n",
            "================\n",
            "0 0.947904634359072\n",
            "210 0.9461279124701055\n",
            "420 0.9436400561384007\n",
            "630 0.9416127385458427\n",
            "840 0.9389567479002078\n",
            "Epoch 31 finished, loss : 2.1001662\n",
            " информации и международных отношений центральной тибетской администрации его святейшества далай - ламы <END>  информации и международных отношений отношений центральной отношений святейшества святейшества далай далай далай далай\n",
            "----------------\n",
            " в конце пять , , . . . <END>\n",
            "================\n",
            "0 0.9422558075248829\n",
            "210 0.9406650651629582\n",
            "420 0.93889611203193\n",
            "630 0.9370930229996949\n",
            "840 0.9344097691985279\n",
            "Epoch 32 finished, loss : 1.4591991\n",
            " прибыли двадцать психологов , чтобы оказать помощь родственникам шахтеров . <END> <END>  прибыли , психологов , чтобы оказать помощь родственникам . . <END> <END>\n",
            "----------------\n",
            " в конце пять , , , <END>\n",
            "================\n",
            "0 0.9375072696921046\n",
            "210 0.935279277838497\n",
            "420 0.9337305923445062\n",
            "630 0.9314330860183255\n",
            "840 0.9286300714324072\n",
            "Epoch 33 finished, loss : 0.8433698\n",
            " составляет пятьсот пятьдесят одну тысячу пятьсот квадратных километров ( шестьсот семьдесят пять тысяч четыреста семнадцать квадратных километров вместе с заморскими владениями <END>  составляет вместе ( шестьсот семьдесят пять тысяч четыреста семнадцать квадратных километров вместе с заморскими вместе <END>\n",
            "----------------\n",
            " в конце пять , но . <END>\n",
            "================\n",
            "0 0.9321068082473302\n",
            "210 0.9306521135626786\n",
            "420 0.9285166454887464\n",
            "630 0.9261771610611501\n",
            "840 0.9238656918922408\n",
            "Epoch 34 finished, loss : 1.4360293\n",
            " севернее дудинки и северо - восточнее белочи , в низменной долине неруссы  севернее дудинки и северо - - , северо - северо в <END>\n",
            "----------------\n",
            " в конце ; , , ее . . <END>\n",
            "================\n",
            "0 0.9270251739697616\n",
            "210 0.9255280311105267\n",
            "420 0.9239823442418572\n",
            "630 0.9218868798193322\n",
            "840 0.9199257238331445\n",
            "Epoch 35 finished, loss : 0.2335386\n",
            " является зеркальным отображением его лицевой <END>  является зеркальным отображением его лицевой <END>\n",
            "----------------\n",
            " в разных ; , . . . . <END>\n",
            "================\n",
            "0 0.9230481878899451\n",
            "210 0.9221470039787352\n",
            "420 0.9199013765260087\n",
            "630 0.9175646092632013\n",
            "840 0.9153052670969959\n",
            "Epoch 36 finished, loss : 0.7614913\n",
            " курса филол . <END> <END>  курса курса . <END> <END>\n",
            "----------------\n",
            " в конце ; , . . . . <END>\n",
            "================\n",
            "0 0.9180230108542152\n",
            "210 0.9164255717501365\n",
            "420 0.9146262177808557\n",
            "630 0.9129203553446008\n",
            "840 0.9105850540315744\n",
            "Epoch 37 finished, loss : 0.2407530\n",
            " в. в. маланин — пермь : <END>  в. в. маланин — пермь : <END>\n",
            "----------------\n",
            " в конце ландштрассе , , ее . . <END>\n",
            "================\n",
            "0 0.913851829391039\n",
            "210 0.9122654498648044\n",
            "420 0.910241189081826\n",
            "630 0.9084987168303772\n",
            "840 0.906487675464562\n",
            "Epoch 38 finished, loss : 0.0281621\n",
            " его длине — <END>  его длине — <END>\n",
            "----------------\n",
            " в конце , , , . . . <END>\n",
            "================\n",
            "0 0.9099689339628917\n",
            "210 0.9086823536562669\n",
            "420 0.9070868993594408\n",
            "630 0.9053109967052311\n",
            "840 0.9036209537339369\n",
            "Epoch 39 finished, loss : 0.0711304\n",
            " польши преимущественно малоплодородны <END>  польши преимущественно малоплодородны <END>\n",
            "----------------\n",
            " в конце ; , , , . . . <END>\n",
            "================\n",
            "0 0.9070711468008124\n",
            "210 0.9053572882373144\n",
            "420 0.9037506508791071\n",
            "630 0.9014223160276355\n",
            "840 0.8996894989308598\n",
            "Epoch 40 finished, loss : 1.1499105\n",
            " в ожидании подхода стрелковых соединений шестой армии  в ожидании соединений стрелковых соединений шестой <END>\n",
            "----------------\n",
            " в разных , , , . — <END>\n",
            "================\n",
            "0 0.9030419291058593\n",
            "210 0.901374547290785\n",
            "420 0.9000228934678703\n",
            "630 0.8982236867038472\n",
            "840 0.896152510070949\n",
            "Epoch 41 finished, loss : 1.5226574\n",
            " в найроби была начата операция « наковальня » , город был поставлен под <END>  в найроби была город операция « наковальня » , город район , под поставлен\n",
            "----------------\n",
            " в конце , , , . . <END>\n",
            "================\n",
            "0 0.8999698115291809\n",
            "210 0.8982304172544652\n",
            "420 0.8968466770218984\n",
            "630 0.8946915458025767\n",
            "840 0.8928935414588246\n",
            "Epoch 42 finished, loss : 0.1694186\n",
            " сд ) и непосредственно <END>  сд ) и непосредственно <END>\n",
            "----------------\n",
            " в конце , , когда . . <END>\n",
            "================\n",
            "0 0.8954263337930048\n",
            "210 0.8943814219957124\n",
            "420 0.8928302206748261\n",
            "630 0.8913745948889785\n",
            "840 0.8897734432922658\n",
            "Epoch 43 finished, loss : 0.3885662\n",
            " , три органа управления существуют <END>  , три органа органа существуют <END>\n",
            "----------------\n",
            " в конце четыре , , . . <END>\n",
            "================\n",
            "0 0.892663256953742\n",
            "210 0.8915889668057754\n",
            "420 0.8900294393558094\n",
            "630 0.8884020095508561\n",
            "840 0.8860327931034088\n",
            "Epoch 44 finished, loss : 1.7680227\n",
            " маршрут проектируемой автодороги « вилюй » , в настоящее время фактически не <END>  автодороги проектируемой автодороги « вилюй вилюй , фактически фактически фактически фактически фактически фактически\n",
            "----------------\n",
            " в конце , , , . <END>\n",
            "================\n",
            "0 0.8893365170246589\n",
            "210 0.8882008460212376\n",
            "420 0.8867144159525988\n",
            "630 0.8849277499781235\n",
            "840 0.883024238273671\n",
            "Epoch 45 finished, loss : 0.3167011\n",
            " — с. сто девяносто восемь . — двенадцать тысяч пятьсот <END>  — с. сто девяносто восемь . — двенадцать тысяч пятьсот двенадцать тысяч пятьсот\n",
            "----------------\n",
            " в конце , , , затем . <END>\n",
            "================\n",
            "0 0.8863506187188304\n",
            "210 0.8850184343372016\n",
            "420 0.883165652224397\n",
            "630 0.8813501077935018\n",
            "840 0.8794994253537828\n",
            "Epoch 46 finished, loss : 0.2712414\n",
            " коллежские советники . <END>  коллежские советники . <END>\n",
            "----------------\n",
            " в конце , , , он . <END>\n",
            "================\n",
            "0 0.8820247360174669\n",
            "210 0.8804594533787853\n",
            "420 0.8787212280416249\n",
            "630 0.8770581998729986\n",
            "840 0.875318173356588\n",
            "Epoch 47 finished, loss : 2.6817640\n",
            " согласен и оператор сергей астахов <END>  согласен третьей в съемках третьей <END>\n",
            "----------------\n",
            " в конце , , . вынужден <END>\n",
            "================\n",
            "0 0.8782556346325341\n",
            "210 0.8767628227252839\n",
            "420 0.8754713120353825\n",
            "630 0.874270892355607\n",
            "840 0.8717106718770038\n",
            "Epoch 48 finished, loss : 0.1192245\n",
            " четырнадцатого августа две тысячи одиннадцатого года . <END>  четырнадцатого августа две тысячи одиннадцатого года . <END>\n",
            "----------------\n",
            " в конце совета , , . . <END>\n",
            "================\n",
            "0 0.8748723091439929\n",
            "210 0.8734610167773091\n",
            "420 0.8715447210773722\n",
            "630 0.8697011493759617\n",
            "840 0.8678031792281141\n",
            "Epoch 49 finished, loss : 1.1235395\n",
            " - выер » ( полная версия ) юрий коваль <END>  - выер » ( полная ) <END> <END> <END> <END>\n",
            "----------------\n",
            " в конце . , , . <END>\n",
            "================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc5qZqJmBs3k",
        "outputId": "3c22a708-32e1-4919-9a53-07ec7084f12f"
      },
      "source": [
        "print(predict(['<START>', 'в', 'конце', '1811 года', ',', 'вследствие', 'конфликта', '.', '<END>'])[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " в конце . , , . <END>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "er2_fP3FBs05",
        "outputId": "eff07a14-9233-4a53-ecfb-7ef9737e5db9"
      },
      "source": [
        "output_words, attentions = predict(dataset[13])\n",
        "plt.matshow(attentions)\n",
        "print(output_words)\n",
        "plt.show()\n",
        "attentions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " поражение тысяча девятьсот пятого года потерпела поражение . <END>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 336x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0., 1., 0., 0., 0., 0., 0.], dtype=float32),\n",
              " array([0., 0., 1., 0., 0., 0., 0.], dtype=float32),\n",
              " array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
              "        6.0674894e-36, 0.0000000e+00, 0.0000000e+00], dtype=float32),\n",
              " array([0.000000e+00, 0.000000e+00, 0.000000e+00, 3.624347e-37,\n",
              "        1.000000e+00, 1.732397e-39, 3.174530e-40], dtype=float32),\n",
              " array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
              "        9.9610537e-01, 7.6938467e-15, 3.8945857e-03], dtype=float32),\n",
              " array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
              "        0.0000000e+00, 2.3143187e-22, 1.0000000e+00], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hyE_wi3MYR_"
      },
      "source": [
        "> Оооо, attentions начали работать корректно, ура."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOQlIpiJOB_D"
      },
      "source": [
        "def predict(seq):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    seq = prepare_sent(seq)\n",
        "    predicted_target = []\n",
        "    att = []\n",
        "    with torch.no_grad():\n",
        "        X = seq\n",
        "        X = torch.tensor(X, device=device).view(1, -1)\n",
        "        # y = torch.tensor(y, device=device).view(1, -1)\n",
        "        hidden = encoder.initHidden()\n",
        "        output_E, hidden = encoder(X, hidden)\n",
        "        # output_E : seq x 1 x hidden_size <-- need reshape\n",
        "        # hidden : 1 x 1 x hidden_size\n",
        "        output_E = torch.swapaxes(output_E, 0, 1)\n",
        "        output_E = torch.swapaxes(output_E, 1, 2)\n",
        "        # output_E : 1 x hidden_size x seq\n",
        "        # true_target = ''\n",
        "        # print(len(y) - 1)\n",
        "        token = torch.tensor([data.word2idx['<START>']], device=device) \n",
        "        word_hidden = decoder.initHidden()\n",
        "        for i in range(228):\n",
        "            token = token.to(device).view(1, 1, 1)\n",
        "\n",
        "            probas, hidden, att_w, word_hidden = decoder(token, hidden, output_E, word_hidden)\n",
        "            att.append(torch.squeeze(att_w).detach().cpu().numpy())\n",
        "            # print(probas.view(1, -1).shape)\n",
        "            # loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "            # true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "            idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "            predicted_target += [data.idx2word[idx_pred]]\n",
        "            token = torch.tensor([idx_pred], device=device)\n",
        "            if idx_pred == end_token_id:\n",
        "                break\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    return predicted_target, att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YccbknsLBsyW"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions, cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence, rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "Uask039OBsvQ",
        "outputId": "b3750c4b-1e8c-4cc3-ec8e-85ff989b4e1e"
      },
      "source": [
        "output_words, attentions = predict(dataset[13])\n",
        "print(output_words)\n",
        "showAttention(dataset[13], output_words, attentions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['поражение', 'тысяча девятьсот пятого года', 'потерпела', 'поражение', '.', '<END>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYo-BEKXkr00"
      },
      "source": [
        "> С предобученным word2vec было бы лучше, наверное. Но на векторизацию тратится очень много времени. Из-за этого 1 эпоха обучения ~ 500 минут. Поэтому результата в ячейках ниже нет существенного. Ладно. Основная цель -- разобраться с seq2seq и Attention выполнена."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj0yKkNUmQ1i"
      },
      "source": [
        "### Безуспешные попытки прикрутить pretrained word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WnxCV8lBsuG",
        "outputId": "da4be051-9f49-45ae-a0be-5f107d7c7e40"
      },
      "source": [
        "!pip install simple-elmo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting simple-elmo\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/56/2382ba23451357a680831669f180de7c8ffc34fd62c71536d38abc068e40/simple_elmo-0.8.0-py3-none-any.whl (45kB)\n",
            "\r\u001b[K     |███████▏                        | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 20kB 19.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 30kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 40kB 20.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>1.8.1 in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (5.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (1.1.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simple-elmo) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simple-elmo) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->simple-elmo) (1.15.0)\n",
            "Installing collected packages: simple-elmo\n",
            "Successfully installed simple-elmo-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "lintGOixBsqI",
        "outputId": "4a9d787c-6cde-4799-8c5b-5262e3f24b72"
      },
      "source": [
        "from simple_elmo import ElmoModel\n",
        "emb_model = ElmoModel()\n",
        "emb_model.load('/content/drive/MyDrive/195(elmo).zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-14 10:16:06,805 : INFO : Loading model from /content/drive/MyDrive/195(elmo).zip...\n",
            "2021-05-14 10:16:06,808 : INFO : \n",
            "            Assuming the model is a ZIP archive downloaded from the NLPL vector repository.\n",
            "            Loading a model from a ZIP archive directly is slower than from the extracted files,\n",
            "            but does not require additional disk space\n",
            "            and allows to load from directories without write permissions.\n",
            "            \n",
            "2021-05-14 10:16:06,830 : INFO : We will cache the vocabulary of 100 tokens.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:903: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The model is now loaded.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCWRrPnPBsnC",
        "outputId": "8440eb63-9ea0-4fb9-a986-e16d1bfdecf5"
      },
      "source": [
        "emb_model.get_elmo_vector_average(['привет', 'кто', 'я', '<START>', '.']).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-14 10:18:42,839 : INFO : Warming up ELMo on 5 sentences...\n",
            "2021-05-14 10:18:45,676 : INFO : Warming up finished.\n",
            "2021-05-14 10:18:45,678 : INFO : Texts in the current batch: 5\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ritG5Yk_TheP"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.emb = torch.nn.Embedding(len(data.idx2word), embedding_dim)\n",
        "        self.rnn = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, dropout=0.6)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = x.view(-1, 1, self.embedding_dim) # (seq_len = -1, batch = 1, input_size = embeddin_dim)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQKdhzOlTha1"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.emb = torch.nn.Embedding(len(data.idx2word), embedding_dim)\n",
        "        self.word_rnn = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, dropout=0.6)\n",
        "        self.rnn = torch.nn.GRU(input_size=hidden_size + hidden_size, hidden_size=hidden_size, dropout=0.6)\n",
        "        self.W = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.drop = torch.nn.Dropout(0.6)\n",
        "        self.FC = torch.nn.Linear(hidden_size, len(data.idx2word))\n",
        "\n",
        "    def forward(self, token, hidden, enc_out, word_hidden): # here goes only one token\n",
        "        # token : (1, 1, 1)\n",
        "        # hidden : (1, 1, hidden_size)\n",
        "        # enc_out : (1, hidden_size, enc_input)\n",
        "        embedded = token.view(1, 1, self.embedding_dim) # v_i\n",
        "        out, word = self.word_rnn(embedded, word_hidden)\n",
        "        att_w = F.softmax(torch.transpose(torch.bmm(self.drop(self.W(word)), enc_out), 1, 2), dim=1) # (batch, enc_seq_out, 1)\n",
        "        att = torch.bmm(enc_out, att_w).view(1, 1, -1) # (1, 1, hidden)\n",
        "        x = torch.cat((att, word), dim=2) # (1, 1, hidden + embedding)\n",
        "        output, hidden = self.rnn(F.silu(x), hidden)\n",
        "        probas = self.FC(hidden)\n",
        "        return probas, hidden, att_w, word\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXd9RafVPim"
      },
      "source": [
        "def predict(seq):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    seq = prepare_sent(seq)\n",
        "    predicted_target = []\n",
        "    att = []\n",
        "    with torch.no_grad():\n",
        "        X = seq\n",
        "        X = emb_model.get_elmo_vector_average(data.detokenize(X))\n",
        "\n",
        "        X = torch.FloatTensor(X).to(device) # seq_len x 1024\n",
        "        \n",
        "        # y = torch.tensor(y, device=device).view(1, -1)\n",
        "        hidden = encoder.initHidden()\n",
        "        output_E, hidden = encoder(X, hidden)\n",
        "        # output_E : seq x 1 x hidden_size <-- need reshape\n",
        "        # hidden : 1 x 1 x hidden_size\n",
        "        output_E = torch.swapaxes(output_E, 0, 1)\n",
        "        output_E = torch.swapaxes(output_E, 1, 2)\n",
        "        # output_E : 1 x hidden_size x seq\n",
        "        # true_target = ''\n",
        "        # print(len(y) - 1)\n",
        "        word_hidden = decoder.initHidden()\n",
        "        token = torch.FloatTensor(emb_model.get_elmo_vector_average(['<START>'])).to(device)\n",
        "        for i in range(228):\n",
        "            token = token.view(1, 1, 1024)\n",
        "            probas, hidden, att_w, word_hidden = decoder(token, hidden, output_E, word_hidden)\n",
        "            att.append(torch.squeeze(att_w).detach().cpu().numpy())\n",
        "            # print(probas.view(1, -1).shape)\n",
        "            # loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "            # true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "            idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "            predicted_target += [data.idx2word[idx_pred]]\n",
        "            \n",
        "            token = torch.FloatTensor(emb_model.get_elmo_vector_average(data.detokenize([idx_pred]))).to(device)\n",
        "            if idx_pred == end_token_id:\n",
        "                break\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    return predicted_target, att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1OVRnP6ThYR",
        "outputId": "4a574f4e-f0c4-4b23-c4f4-56271fe8aa43"
      },
      "source": [
        "encoder = EncoderRNN(1024, 256).to(device)\n",
        "decoder = DecoderRNN(1024, 256).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vVEwTUeThV8"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
        "optimizer_D = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
        "EPOCHS = 1 # 60 epochs is +- enough\n",
        "teacher_forcing_ratio = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYmr1ExAThT4"
      },
      "source": [
        "hist_losses = []\n",
        "end_token_id = data.word2idx['<END>']\n",
        "for epoch in range(EPOCHS):\n",
        "    permutation = np.random.permutation(len(data))\n",
        "    true_target = []\n",
        "    predicted_target = []\n",
        "    for j in range(len(data)):\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "        X, y = data[permutation[j]]\n",
        "\n",
        "        # Take random segment\n",
        "        sz = len(X)\n",
        "        r_bound = random.randint(max(2, sz // 2), sz - 1) + 1\n",
        "        l_bound = random.randint(1, r_bound - 1)\n",
        "        X = [data.word2idx['<START>']] + X[l_bound:r_bound] + [data.word2idx['<END>']]\n",
        "        y = [data.word2idx['<START>']] + y[l_bound:r_bound] + [data.word2idx['<END>']]\n",
        "        \n",
        "        X = emb_model.get_elmo_vector_average(data.detokenize(X), warmup=False)\n",
        "\n",
        "        X = torch.FloatTensor(X).to(device) # seq_len x 1024\n",
        "        # y = torch.tensor(y, device=device).view(1, -1)\n",
        "        hidden = encoder.initHidden()\n",
        "        output_E, hidden = encoder(X, hidden)\n",
        "        # output_E : seq x 1 x hidden_size <-- need reshape\n",
        "        # hidden : 1 x 1 x hidden_size\n",
        "        output_E = torch.swapaxes(output_E, 0, 1)\n",
        "        output_E = torch.swapaxes(output_E, 1, 2)\n",
        "        # output_E : 1 x hidden_size x seq\n",
        "        loss = 0\n",
        "        true_target.append('')\n",
        "        predicted_target.append('')\n",
        "        # print(len(y) - 1)\n",
        "        word_hidden = decoder.initHidden()\n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "        if use_teacher_forcing:\n",
        "            for i in range(len(y) - 1):\n",
        "                token = torch.FloatTensor(emb_model.get_elmo_vector_average(data.detokenize([y[i]]), warmup=False)).to(device)\n",
        "                token = token.view(1, 1, 1024)\n",
        "\n",
        "                probas, hidden, att_w, word_hidden = decoder(token, hidden, output_E, word_hidden)\n",
        "                # print(probas.view(1, -1).shape)\n",
        "                loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "                true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "                idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "                predicted_target[-1] += ' ' + data.idx2word[idx_pred]\n",
        "        else:\n",
        "            token = torch.FloatTensor(emb_model.get_elmo_vector_average(data.detokenize([y[0]]), warmup=False)).to(device)\n",
        "            for i in range(len(y) - 1):\n",
        "                token = torch.FloatTensor(emb_model.get_elmo_vector_average(data.detokenize([y[i]]), warmup=False)).to(device)\n",
        "                token = token.view(1, 1, 1024)\n",
        "\n",
        "                probas, hidden, att_w, word_hidden = decoder(token, hidden, output_E, word_hidden)\n",
        "                # print(probas.view(1, -1).shape)\n",
        "                loss += criterion(probas.view(1, -1), torch.tensor([y[i + 1]], device=device))\n",
        "                true_target[-1] += ' ' + data.idx2word[y[i + 1]]\n",
        "                idx_pred = torch.argmax(torch.squeeze(probas)).detach().cpu().item()\n",
        "                predicted_target[-1] += ' ' + data.idx2word[idx_pred]\n",
        "                token = torch.tensor([idx_pred], device=device)\n",
        "                if idx_pred == end_token_id:\n",
        "                    break\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "        optimizer_E.step()\n",
        "        hist_losses.append(loss.detach().cpu().item() / (len(y) - 1))\n",
        "        if (j % 210 == 0):\n",
        "            print(j, np.mean(hist_losses[max(0, j - 42):-1]))\n",
        "    print(f'Epoch {epoch} finished, loss : {hist_losses[-1]:.7f}')\n",
        "    for i in range(1, 2):\n",
        "        print(true_target[-i], predicted_target[-i])\n",
        "    print('----------------')\n",
        "    print(predict(['<START>', 'в', 'конце', '1811 года', ',', 'вследствие', 'конфликта', '.', '<END>'])[0])\n",
        "    print('================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "AlTErSXIThRu",
        "outputId": "f1085fda-f5f0-447f-8178-e603dd5abb91"
      },
      "source": [
        "output_words, attentions = predict(dataset[13])\n",
        "print(output_words)\n",
        "showAttention(dataset[13], output_words, attentions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-14 11:06:51,996 : INFO : Warming up ELMo on 7 sentences...\n",
            "2021-05-14 11:06:52,390 : INFO : Warming up finished.\n",
            "2021-05-14 11:06:52,392 : INFO : Texts in the current batch: 7\n",
            "2021-05-14 11:06:54,466 : INFO : Warming up ELMo on 1 sentences...\n",
            "2021-05-14 11:06:54,864 : INFO : Warming up finished.\n",
            "2021-05-14 11:06:54,865 : INFO : Texts in the current batch: 1\n",
            "2021-05-14 11:06:57,045 : INFO : Warming up ELMo on 1 sentences...\n",
            "2021-05-14 11:06:57,447 : INFO : Warming up finished.\n",
            "2021-05-14 11:06:57,449 : INFO : Texts in the current batch: 1\n",
            "2021-05-14 11:06:59,649 : INFO : Warming up ELMo on 1 sentences...\n",
            "2021-05-14 11:07:00,020 : INFO : Warming up finished.\n",
            "2021-05-14 11:07:00,021 : INFO : Texts in the current batch: 1\n",
            "2021-05-14 11:07:02,104 : INFO : Warming up ELMo on 1 sentences...\n",
            "2021-05-14 11:07:02,465 : INFO : Warming up finished.\n",
            "2021-05-14 11:07:02,466 : INFO : Texts in the current batch: 1\n",
            "2021-05-14 11:07:04,545 : INFO : Warming up ELMo on 1 sentences...\n",
            "2021-05-14 11:07:04,905 : INFO : Warming up finished.\n",
            "2021-05-14 11:07:04,906 : INFO : Texts in the current batch: 1\n",
            "2021-05-14 11:07:07,151 : INFO : Warming up ELMo on 1 sentences...\n",
            "2021-05-14 11:07:07,551 : INFO : Warming up finished.\n",
            "2021-05-14 11:07:07,553 : INFO : Texts in the current batch: 1\n",
            "2021-05-14 11:07:09,630 : INFO : Warming up ELMo on 1 sentences...\n",
            "2021-05-14 11:07:10,005 : INFO : Warming up finished.\n",
            "2021-05-14 11:07:10,007 : INFO : Texts in the current batch: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['в', ',', ',', ',', ',', '<END>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}